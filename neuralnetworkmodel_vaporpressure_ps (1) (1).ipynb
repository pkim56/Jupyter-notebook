{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): failed\n",
      "\n",
      "NoSpaceLeftError: No space left on devices.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge keras --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - conda\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    absl-py-0.13.0             |   py37hecd8cb5_0         176 KB\n",
      "    astor-0.8.1                |   py37hecd8cb5_0          47 KB\n",
      "    backports.functools_lru_cache-1.6.4|     pyhd3eb1b0_0           9 KB\n",
      "    backports.tempfile-1.0     |     pyhd3eb1b0_1          11 KB\n",
      "    c-ares-1.17.1              |       h9ed2024_0          92 KB\n",
      "    conda-4.10.3               |   py37hecd8cb5_0         2.9 MB\n",
      "    conda-package-handling-1.7.3|   py37h9ed2024_1         1.5 MB\n",
      "    gast-0.5.0                 |     pyhd3eb1b0_0          13 KB\n",
      "    google-pasta-0.2.0         |             py_0          46 KB\n",
      "    importlib-metadata-3.10.0  |   py37hecd8cb5_0          33 KB\n",
      "    keras-applications-1.0.8   |             py_1          29 KB\n",
      "    keras-preprocessing-1.1.2  |     pyhd3eb1b0_0          35 KB\n",
      "    llvm-4.0.1                 |                1        43.0 MB\n",
      "    markdown-3.3.4             |   py37hecd8cb5_0         127 KB\n",
      "    termcolor-1.1.0            |   py37hecd8cb5_1           9 KB\n",
      "    theano-1.0.4               |   py37h04f5b5a_0         3.1 MB\n",
      "    typing_extensions-3.10.0.0 |     pyh06a4308_0          27 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        51.1 MB\n",
      "\n",
      "The following packages will be REMOVED:\n",
      "\n",
      "  libgpuarray-0.7.6-h0d85af4_1003\n",
      "  mako-1.1.4-pyh44b312d_0\n",
      "  pygpu-0.7.6-py37h9f9e6dc_1002\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  backports.functoo~                             1.6.1-py_0 --> 1.6.4-pyhd3eb1b0_0\n",
      "  conda-package-han~                   1.6.1-py37h1de35cc_0 --> 1.7.3-py37h9ed2024_1\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  absl-py            conda-forge/noarch::absl-py-0.13.0-py~ --> pkgs/main/osx-64::absl-py-0.13.0-py37hecd8cb5_0\n",
      "  astor              conda-forge/noarch::astor-0.8.1-pyh9f~ --> pkgs/main/osx-64::astor-0.8.1-py37hecd8cb5_0\n",
      "  c-ares              conda-forge::c-ares-1.17.1-h0d85af4_1 --> pkgs/main::c-ares-1.17.1-h9ed2024_0\n",
      "  conda              conda-forge::conda-4.10.3-py37hf98548~ --> pkgs/main::conda-4.10.3-py37hecd8cb5_0\n",
      "  gast                 conda-forge::gast-0.5.0-pyhd8ed1ab_0 --> pkgs/main::gast-0.5.0-pyhd3eb1b0_0\n",
      "  google-pasta       conda-forge::google-pasta-0.2.0-pyh8c~ --> pkgs/main::google-pasta-0.2.0-py_0\n",
      "  importlib-metadata conda-forge::importlib-metadata-4.6.1~ --> pkgs/main::importlib-metadata-3.10.0-py37hecd8cb5_0\n",
      "  keras-applications                            conda-forge --> pkgs/main\n",
      "  keras-preprocessi~ conda-forge::keras-preprocessing-1.1.~ --> pkgs/main::keras-preprocessing-1.1.2-pyhd3eb1b0_0\n",
      "  llvm                                          conda-forge --> pkgs/main\n",
      "  markdown           conda-forge/noarch::markdown-3.3.4-py~ --> pkgs/main/osx-64::markdown-3.3.4-py37hecd8cb5_0\n",
      "  termcolor          conda-forge/noarch::termcolor-1.1.0-p~ --> pkgs/main/osx-64::termcolor-1.1.0-py37hecd8cb5_1\n",
      "  theano             conda-forge::theano-1.0.4-py37h0a4402~ --> pkgs/main::theano-1.0.4-py37h04f5b5a_0\n",
      "  typing_extensions  conda-forge::typing_extensions-3.10.0~ --> pkgs/main::typing_extensions-3.10.0.0-pyh06a4308_0\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "  backports.tempfile                               1.0-py_1 --> 1.0-pyhd3eb1b0_1\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? ^C\n",
      "\n",
      "CondaSystemExit: \n",
      "Operation aborted.  Exiting.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda update -n base -c defaults conda\n",
    "!conda install -c conda-forge tensorflow --yes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.datasets import imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data( num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import r2_score \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.7/site-packages (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/anaconda3/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/anaconda3/lib/python3.7/site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/anaconda3/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Psi_i_t</th>\n",
       "      <th>MPC06</th>\n",
       "      <th>SpMaxA_X</th>\n",
       "      <th>VE1_B(m)</th>\n",
       "      <th>SpMax6_Bh(s)</th>\n",
       "      <th>SpMaxA_AEA(ri)</th>\n",
       "      <th>SM11_AEA(bo)</th>\n",
       "      <th>Eig03_EA</th>\n",
       "      <th>Eig03_EA(ri)</th>\n",
       "      <th>Eig04_AEA(ed)</th>\n",
       "      <th>Eig04_AEA(bo)</th>\n",
       "      <th>Eig04_AEA(dm)</th>\n",
       "      <th>Eig03_AEA(ri)</th>\n",
       "      <th>Eig04_AEA(ri)</th>\n",
       "      <th>VaporPressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.054</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.143</td>\n",
       "      <td>2.464</td>\n",
       "      <td>2.318</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.232</td>\n",
       "      <td>1.294</td>\n",
       "      <td>0.405</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>1.134</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.931659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.071</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.167</td>\n",
       "      <td>2.327</td>\n",
       "      <td>2.073</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.659</td>\n",
       "      <td>0.867</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.918682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.115</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>2.156</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.636</td>\n",
       "      <td>-0.618</td>\n",
       "      <td>-0.618</td>\n",
       "      <td>-0.541</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.618</td>\n",
       "      <td>-1.328</td>\n",
       "      <td>0.257</td>\n",
       "      <td>-0.697</td>\n",
       "      <td>0.990197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.143</td>\n",
       "      <td>2.499</td>\n",
       "      <td>2.027</td>\n",
       "      <td>0.561</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>-0.388</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.596</td>\n",
       "      <td>0.543</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.860331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2.996</td>\n",
       "      <td>0.077</td>\n",
       "      <td>3.326</td>\n",
       "      <td>3.889</td>\n",
       "      <td>0.314</td>\n",
       "      <td>1.646</td>\n",
       "      <td>1.646</td>\n",
       "      <td>1.625</td>\n",
       "      <td>3.759</td>\n",
       "      <td>2.862</td>\n",
       "      <td>1.610</td>\n",
       "      <td>2.634</td>\n",
       "      <td>2.414</td>\n",
       "      <td>0.018548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.128</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1.933</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.781</td>\n",
       "      <td>-1.414</td>\n",
       "      <td>-1.414</td>\n",
       "      <td>-1.304</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.860237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.008</td>\n",
       "      <td>2.197</td>\n",
       "      <td>0.100</td>\n",
       "      <td>2.992</td>\n",
       "      <td>3.244</td>\n",
       "      <td>0.389</td>\n",
       "      <td>1.414</td>\n",
       "      <td>1.414</td>\n",
       "      <td>1.397</td>\n",
       "      <td>3.466</td>\n",
       "      <td>2.607</td>\n",
       "      <td>1.319</td>\n",
       "      <td>2.403</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.126692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3.332</td>\n",
       "      <td>0.071</td>\n",
       "      <td>3.214</td>\n",
       "      <td>3.930</td>\n",
       "      <td>0.293</td>\n",
       "      <td>2.125</td>\n",
       "      <td>2.125</td>\n",
       "      <td>1.931</td>\n",
       "      <td>3.759</td>\n",
       "      <td>2.426</td>\n",
       "      <td>2.286</td>\n",
       "      <td>3.056</td>\n",
       "      <td>1.904</td>\n",
       "      <td>0.008832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.035</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.167</td>\n",
       "      <td>2.225</td>\n",
       "      <td>1.115</td>\n",
       "      <td>0.698</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.593</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>0.897022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.007</td>\n",
       "      <td>2.197</td>\n",
       "      <td>0.100</td>\n",
       "      <td>2.950</td>\n",
       "      <td>3.296</td>\n",
       "      <td>0.391</td>\n",
       "      <td>1.414</td>\n",
       "      <td>1.414</td>\n",
       "      <td>1.376</td>\n",
       "      <td>3.466</td>\n",
       "      <td>2.532</td>\n",
       "      <td>1.311</td>\n",
       "      <td>2.392</td>\n",
       "      <td>2.005</td>\n",
       "      <td>0.026860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.019</td>\n",
       "      <td>1.792</td>\n",
       "      <td>0.091</td>\n",
       "      <td>3.018</td>\n",
       "      <td>3.030</td>\n",
       "      <td>0.278</td>\n",
       "      <td>1.310</td>\n",
       "      <td>1.310</td>\n",
       "      <td>1.052</td>\n",
       "      <td>2.618</td>\n",
       "      <td>1.831</td>\n",
       "      <td>1.767</td>\n",
       "      <td>2.151</td>\n",
       "      <td>1.618</td>\n",
       "      <td>0.014070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2.890</td>\n",
       "      <td>0.077</td>\n",
       "      <td>3.026</td>\n",
       "      <td>3.538</td>\n",
       "      <td>0.293</td>\n",
       "      <td>1.591</td>\n",
       "      <td>1.591</td>\n",
       "      <td>1.519</td>\n",
       "      <td>3.624</td>\n",
       "      <td>2.910</td>\n",
       "      <td>1.562</td>\n",
       "      <td>2.548</td>\n",
       "      <td>2.450</td>\n",
       "      <td>0.017207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.002</td>\n",
       "      <td>3.258</td>\n",
       "      <td>0.067</td>\n",
       "      <td>3.154</td>\n",
       "      <td>3.736</td>\n",
       "      <td>0.257</td>\n",
       "      <td>2.064</td>\n",
       "      <td>2.064</td>\n",
       "      <td>2.027</td>\n",
       "      <td>4.362</td>\n",
       "      <td>2.919</td>\n",
       "      <td>2.176</td>\n",
       "      <td>3.058</td>\n",
       "      <td>2.521</td>\n",
       "      <td>0.005477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.004</td>\n",
       "      <td>2.773</td>\n",
       "      <td>0.091</td>\n",
       "      <td>3.039</td>\n",
       "      <td>3.432</td>\n",
       "      <td>0.348</td>\n",
       "      <td>1.414</td>\n",
       "      <td>1.414</td>\n",
       "      <td>1.216</td>\n",
       "      <td>3.775</td>\n",
       "      <td>2.818</td>\n",
       "      <td>1.932</td>\n",
       "      <td>2.309</td>\n",
       "      <td>2.202</td>\n",
       "      <td>0.005477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.013</td>\n",
       "      <td>2.079</td>\n",
       "      <td>0.111</td>\n",
       "      <td>2.778</td>\n",
       "      <td>2.840</td>\n",
       "      <td>0.416</td>\n",
       "      <td>1.414</td>\n",
       "      <td>1.414</td>\n",
       "      <td>1.346</td>\n",
       "      <td>2.415</td>\n",
       "      <td>1.459</td>\n",
       "      <td>0.739</td>\n",
       "      <td>2.371</td>\n",
       "      <td>1.065</td>\n",
       "      <td>0.782280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.003</td>\n",
       "      <td>3.664</td>\n",
       "      <td>0.062</td>\n",
       "      <td>3.512</td>\n",
       "      <td>3.467</td>\n",
       "      <td>0.278</td>\n",
       "      <td>2.593</td>\n",
       "      <td>2.593</td>\n",
       "      <td>2.477</td>\n",
       "      <td>4.667</td>\n",
       "      <td>2.916</td>\n",
       "      <td>1.769</td>\n",
       "      <td>3.552</td>\n",
       "      <td>2.566</td>\n",
       "      <td>0.054617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.013</td>\n",
       "      <td>2.197</td>\n",
       "      <td>0.083</td>\n",
       "      <td>3.048</td>\n",
       "      <td>3.467</td>\n",
       "      <td>0.369</td>\n",
       "      <td>1.284</td>\n",
       "      <td>1.284</td>\n",
       "      <td>1.317</td>\n",
       "      <td>3.028</td>\n",
       "      <td>2.089</td>\n",
       "      <td>1.193</td>\n",
       "      <td>2.315</td>\n",
       "      <td>1.941</td>\n",
       "      <td>0.149258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.005</td>\n",
       "      <td>2.890</td>\n",
       "      <td>0.083</td>\n",
       "      <td>3.018</td>\n",
       "      <td>3.263</td>\n",
       "      <td>0.333</td>\n",
       "      <td>1.707</td>\n",
       "      <td>1.707</td>\n",
       "      <td>1.521</td>\n",
       "      <td>3.347</td>\n",
       "      <td>1.917</td>\n",
       "      <td>1.652</td>\n",
       "      <td>2.620</td>\n",
       "      <td>1.678</td>\n",
       "      <td>0.071142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.036</td>\n",
       "      <td>1.792</td>\n",
       "      <td>0.125</td>\n",
       "      <td>2.670</td>\n",
       "      <td>1.889</td>\n",
       "      <td>0.442</td>\n",
       "      <td>1.183</td>\n",
       "      <td>1.183</td>\n",
       "      <td>1.139</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.210</td>\n",
       "      <td>2.164</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.773411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.068</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>2.020</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.645</td>\n",
       "      <td>-0.618</td>\n",
       "      <td>-0.618</td>\n",
       "      <td>-0.548</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.496</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.662</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000</td>\n",
       "      <td>3.497</td>\n",
       "      <td>0.059</td>\n",
       "      <td>3.693</td>\n",
       "      <td>4.141</td>\n",
       "      <td>0.268</td>\n",
       "      <td>1.987</td>\n",
       "      <td>1.987</td>\n",
       "      <td>1.960</td>\n",
       "      <td>4.375</td>\n",
       "      <td>3.094</td>\n",
       "      <td>2.175</td>\n",
       "      <td>2.975</td>\n",
       "      <td>2.420</td>\n",
       "      <td>0.018514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.002</td>\n",
       "      <td>2.890</td>\n",
       "      <td>0.083</td>\n",
       "      <td>3.216</td>\n",
       "      <td>3.606</td>\n",
       "      <td>0.322</td>\n",
       "      <td>1.556</td>\n",
       "      <td>1.556</td>\n",
       "      <td>1.546</td>\n",
       "      <td>3.513</td>\n",
       "      <td>2.629</td>\n",
       "      <td>1.595</td>\n",
       "      <td>2.556</td>\n",
       "      <td>2.006</td>\n",
       "      <td>0.024592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.006</td>\n",
       "      <td>2.303</td>\n",
       "      <td>0.100</td>\n",
       "      <td>2.999</td>\n",
       "      <td>3.362</td>\n",
       "      <td>0.377</td>\n",
       "      <td>1.322</td>\n",
       "      <td>1.322</td>\n",
       "      <td>1.298</td>\n",
       "      <td>3.435</td>\n",
       "      <td>2.586</td>\n",
       "      <td>1.182</td>\n",
       "      <td>2.307</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.469144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.009</td>\n",
       "      <td>2.565</td>\n",
       "      <td>0.100</td>\n",
       "      <td>2.964</td>\n",
       "      <td>3.215</td>\n",
       "      <td>0.377</td>\n",
       "      <td>1.235</td>\n",
       "      <td>1.235</td>\n",
       "      <td>1.217</td>\n",
       "      <td>3.414</td>\n",
       "      <td>2.500</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.238</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.024181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.015</td>\n",
       "      <td>2.565</td>\n",
       "      <td>0.100</td>\n",
       "      <td>2.904</td>\n",
       "      <td>2.769</td>\n",
       "      <td>0.362</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.913</td>\n",
       "      <td>2.319</td>\n",
       "      <td>1.065</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.735</td>\n",
       "      <td>0.296144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.001</td>\n",
       "      <td>3.332</td>\n",
       "      <td>0.059</td>\n",
       "      <td>3.364</td>\n",
       "      <td>3.674</td>\n",
       "      <td>0.219</td>\n",
       "      <td>2.008</td>\n",
       "      <td>2.008</td>\n",
       "      <td>1.921</td>\n",
       "      <td>3.865</td>\n",
       "      <td>2.920</td>\n",
       "      <td>1.745</td>\n",
       "      <td>2.974</td>\n",
       "      <td>2.490</td>\n",
       "      <td>0.011726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.006</td>\n",
       "      <td>2.890</td>\n",
       "      <td>0.083</td>\n",
       "      <td>3.036</td>\n",
       "      <td>3.470</td>\n",
       "      <td>0.319</td>\n",
       "      <td>1.607</td>\n",
       "      <td>1.607</td>\n",
       "      <td>1.466</td>\n",
       "      <td>3.183</td>\n",
       "      <td>2.295</td>\n",
       "      <td>1.881</td>\n",
       "      <td>2.524</td>\n",
       "      <td>1.939</td>\n",
       "      <td>0.000331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.008</td>\n",
       "      <td>2.890</td>\n",
       "      <td>0.083</td>\n",
       "      <td>3.046</td>\n",
       "      <td>3.376</td>\n",
       "      <td>0.319</td>\n",
       "      <td>1.531</td>\n",
       "      <td>1.531</td>\n",
       "      <td>1.420</td>\n",
       "      <td>3.199</td>\n",
       "      <td>2.655</td>\n",
       "      <td>1.432</td>\n",
       "      <td>2.464</td>\n",
       "      <td>2.096</td>\n",
       "      <td>0.192699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.008</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.111</td>\n",
       "      <td>2.889</td>\n",
       "      <td>3.089</td>\n",
       "      <td>0.413</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.993</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.821</td>\n",
       "      <td>2.007</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.016460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000</td>\n",
       "      <td>4.263</td>\n",
       "      <td>0.037</td>\n",
       "      <td>3.460</td>\n",
       "      <td>4.220</td>\n",
       "      <td>0.149</td>\n",
       "      <td>2.473</td>\n",
       "      <td>2.473</td>\n",
       "      <td>2.365</td>\n",
       "      <td>5.368</td>\n",
       "      <td>3.831</td>\n",
       "      <td>2.710</td>\n",
       "      <td>3.432</td>\n",
       "      <td>3.415</td>\n",
       "      <td>0.440949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.010</td>\n",
       "      <td>3.296</td>\n",
       "      <td>0.091</td>\n",
       "      <td>2.928</td>\n",
       "      <td>3.147</td>\n",
       "      <td>0.324</td>\n",
       "      <td>1.534</td>\n",
       "      <td>1.534</td>\n",
       "      <td>1.439</td>\n",
       "      <td>3.450</td>\n",
       "      <td>2.365</td>\n",
       "      <td>1.986</td>\n",
       "      <td>2.477</td>\n",
       "      <td>2.125</td>\n",
       "      <td>0.113715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.003</td>\n",
       "      <td>2.996</td>\n",
       "      <td>0.067</td>\n",
       "      <td>3.058</td>\n",
       "      <td>3.600</td>\n",
       "      <td>0.262</td>\n",
       "      <td>2.007</td>\n",
       "      <td>2.007</td>\n",
       "      <td>1.922</td>\n",
       "      <td>3.859</td>\n",
       "      <td>2.862</td>\n",
       "      <td>1.704</td>\n",
       "      <td>2.975</td>\n",
       "      <td>2.392</td>\n",
       "      <td>0.085426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.015</td>\n",
       "      <td>3.367</td>\n",
       "      <td>0.071</td>\n",
       "      <td>3.411</td>\n",
       "      <td>3.472</td>\n",
       "      <td>0.297</td>\n",
       "      <td>1.603</td>\n",
       "      <td>1.603</td>\n",
       "      <td>1.600</td>\n",
       "      <td>3.732</td>\n",
       "      <td>2.526</td>\n",
       "      <td>1.608</td>\n",
       "      <td>2.602</td>\n",
       "      <td>2.437</td>\n",
       "      <td>0.073476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.008</td>\n",
       "      <td>2.398</td>\n",
       "      <td>0.091</td>\n",
       "      <td>3.004</td>\n",
       "      <td>3.272</td>\n",
       "      <td>0.358</td>\n",
       "      <td>1.414</td>\n",
       "      <td>1.414</td>\n",
       "      <td>1.397</td>\n",
       "      <td>3.566</td>\n",
       "      <td>2.578</td>\n",
       "      <td>1.461</td>\n",
       "      <td>2.403</td>\n",
       "      <td>2.097</td>\n",
       "      <td>0.149846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000</td>\n",
       "      <td>4.078</td>\n",
       "      <td>0.053</td>\n",
       "      <td>3.761</td>\n",
       "      <td>4.362</td>\n",
       "      <td>0.234</td>\n",
       "      <td>2.580</td>\n",
       "      <td>2.580</td>\n",
       "      <td>2.465</td>\n",
       "      <td>5.287</td>\n",
       "      <td>3.449</td>\n",
       "      <td>2.829</td>\n",
       "      <td>3.537</td>\n",
       "      <td>2.947</td>\n",
       "      <td>0.015246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.036</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.125</td>\n",
       "      <td>2.610</td>\n",
       "      <td>2.497</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.727</td>\n",
       "      <td>1.555</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.103</td>\n",
       "      <td>1.715</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.760993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.012</td>\n",
       "      <td>2.833</td>\n",
       "      <td>0.091</td>\n",
       "      <td>3.034</td>\n",
       "      <td>3.329</td>\n",
       "      <td>0.345</td>\n",
       "      <td>1.287</td>\n",
       "      <td>1.287</td>\n",
       "      <td>1.305</td>\n",
       "      <td>3.414</td>\n",
       "      <td>2.500</td>\n",
       "      <td>1.067</td>\n",
       "      <td>2.313</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.290356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.011</td>\n",
       "      <td>2.303</td>\n",
       "      <td>0.111</td>\n",
       "      <td>2.814</td>\n",
       "      <td>3.297</td>\n",
       "      <td>0.410</td>\n",
       "      <td>1.879</td>\n",
       "      <td>1.879</td>\n",
       "      <td>1.749</td>\n",
       "      <td>2.382</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>2.816</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.054897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.005</td>\n",
       "      <td>1.792</td>\n",
       "      <td>0.091</td>\n",
       "      <td>3.202</td>\n",
       "      <td>3.607</td>\n",
       "      <td>0.327</td>\n",
       "      <td>1.516</td>\n",
       "      <td>1.516</td>\n",
       "      <td>1.510</td>\n",
       "      <td>2.724</td>\n",
       "      <td>1.849</td>\n",
       "      <td>1.203</td>\n",
       "      <td>2.519</td>\n",
       "      <td>1.719</td>\n",
       "      <td>0.009318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.012</td>\n",
       "      <td>2.708</td>\n",
       "      <td>0.083</td>\n",
       "      <td>3.069</td>\n",
       "      <td>3.467</td>\n",
       "      <td>0.385</td>\n",
       "      <td>1.697</td>\n",
       "      <td>1.697</td>\n",
       "      <td>1.728</td>\n",
       "      <td>3.754</td>\n",
       "      <td>2.309</td>\n",
       "      <td>1.572</td>\n",
       "      <td>2.717</td>\n",
       "      <td>2.053</td>\n",
       "      <td>0.006442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2.639</td>\n",
       "      <td>0.067</td>\n",
       "      <td>3.696</td>\n",
       "      <td>4.190</td>\n",
       "      <td>0.252</td>\n",
       "      <td>2.170</td>\n",
       "      <td>2.170</td>\n",
       "      <td>2.084</td>\n",
       "      <td>4.345</td>\n",
       "      <td>3.115</td>\n",
       "      <td>2.701</td>\n",
       "      <td>3.145</td>\n",
       "      <td>2.843</td>\n",
       "      <td>0.019907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.000</td>\n",
       "      <td>2.398</td>\n",
       "      <td>0.062</td>\n",
       "      <td>3.365</td>\n",
       "      <td>4.516</td>\n",
       "      <td>0.214</td>\n",
       "      <td>1.868</td>\n",
       "      <td>1.868</td>\n",
       "      <td>1.628</td>\n",
       "      <td>3.578</td>\n",
       "      <td>2.639</td>\n",
       "      <td>2.319</td>\n",
       "      <td>2.748</td>\n",
       "      <td>2.454</td>\n",
       "      <td>0.020866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Psi_i_t  MPC06  SpMaxA_X  VE1_B(m)  SpMax6_Bh(s)  SpMaxA_AEA(ri)  \\\n",
       "0     0.054  0.000     0.143     2.464         2.318           0.547   \n",
       "1     0.071  0.000     0.167     2.327         2.073           0.540   \n",
       "2     0.115  0.000     0.200     2.156         0.999           0.636   \n",
       "3     0.050  0.000     0.143     2.499         2.027           0.561   \n",
       "4     0.001  2.996     0.077     3.326         3.889           0.314   \n",
       "5     0.128  0.000     0.250     1.933         0.999           0.781   \n",
       "6     0.008  2.197     0.100     2.992         3.244           0.389   \n",
       "7     0.002  3.332     0.071     3.214         3.930           0.293   \n",
       "8     0.035  0.000     0.167     2.225         1.115           0.698   \n",
       "9     0.007  2.197     0.100     2.950         3.296           0.391   \n",
       "10    0.019  1.792     0.091     3.018         3.030           0.278   \n",
       "11    0.005  2.890     0.077     3.026         3.538           0.293   \n",
       "12    0.002  3.258     0.067     3.154         3.736           0.257   \n",
       "13    0.004  2.773     0.091     3.039         3.432           0.348   \n",
       "14    0.013  2.079     0.111     2.778         2.840           0.416   \n",
       "15    0.003  3.664     0.062     3.512         3.467           0.278   \n",
       "16    0.013  2.197     0.083     3.048         3.467           0.369   \n",
       "17    0.005  2.890     0.083     3.018         3.263           0.333   \n",
       "18    0.036  1.792     0.125     2.670         1.889           0.442   \n",
       "19    0.068  0.000     0.200     2.020         0.999           0.645   \n",
       "20    0.000  3.497     0.059     3.693         4.141           0.268   \n",
       "21    0.002  2.890     0.083     3.216         3.606           0.322   \n",
       "22    0.006  2.303     0.100     2.999         3.362           0.377   \n",
       "23    0.009  2.565     0.100     2.964         3.215           0.377   \n",
       "24    0.015  2.565     0.100     2.904         2.769           0.362   \n",
       "25    0.001  3.332     0.059     3.364         3.674           0.219   \n",
       "26    0.006  2.890     0.083     3.036         3.470           0.319   \n",
       "27    0.008  2.890     0.083     3.046         3.376           0.319   \n",
       "28    0.008  1.609     0.111     2.889         3.089           0.413   \n",
       "29    0.000  4.263     0.037     3.460         4.220           0.149   \n",
       "30    0.010  3.296     0.091     2.928         3.147           0.324   \n",
       "31    0.003  2.996     0.067     3.058         3.600           0.262   \n",
       "32    0.015  3.367     0.071     3.411         3.472           0.297   \n",
       "33    0.008  2.398     0.091     3.004         3.272           0.358   \n",
       "34    0.000  4.078     0.053     3.761         4.362           0.234   \n",
       "35    0.036  0.693     0.125     2.610         2.497           0.468   \n",
       "36    0.012  2.833     0.091     3.034         3.329           0.345   \n",
       "37    0.011  2.303     0.111     2.814         3.297           0.410   \n",
       "38    0.005  1.792     0.091     3.202         3.607           0.327   \n",
       "39    0.012  2.708     0.083     3.069         3.467           0.385   \n",
       "40    0.001  2.639     0.067     3.696         4.190           0.252   \n",
       "41    0.000  2.398     0.062     3.365         4.516           0.214   \n",
       "\n",
       "    SM11_AEA(bo)  Eig03_EA  Eig03_EA(ri)  Eig04_AEA(ed)  Eig04_AEA(bo)  \\\n",
       "0          0.274     0.274         0.232          1.294          0.405   \n",
       "1          0.000     0.000         0.000          0.382          0.000   \n",
       "2         -0.618    -0.618        -0.541          0.000         -0.618   \n",
       "3         -0.414    -0.414        -0.388          1.000          0.000   \n",
       "4          1.646     1.646         1.625          3.759          2.862   \n",
       "5         -1.414    -1.414        -1.304          0.000          0.000   \n",
       "6          1.414     1.414         1.397          3.466          2.607   \n",
       "7          2.125     2.125         1.931          3.759          2.426   \n",
       "8         -1.000    -1.000        -0.593          1.000          0.000   \n",
       "9          1.414     1.414         1.376          3.466          2.532   \n",
       "10         1.310     1.310         1.052          2.618          1.831   \n",
       "11         1.591     1.591         1.519          3.624          2.910   \n",
       "12         2.064     2.064         2.027          4.362          2.919   \n",
       "13         1.414     1.414         1.216          3.775          2.818   \n",
       "14         1.414     1.414         1.346          2.415          1.459   \n",
       "15         2.593     2.593         2.477          4.667          2.916   \n",
       "16         1.284     1.284         1.317          3.028          2.089   \n",
       "17         1.707     1.707         1.521          3.347          1.917   \n",
       "18         1.183     1.183         1.139          2.000          1.000   \n",
       "19        -0.618    -0.618        -0.548          0.000         -0.496   \n",
       "20         1.987     1.987         1.960          4.375          3.094   \n",
       "21         1.556     1.556         1.546          3.513          2.629   \n",
       "22         1.322     1.322         1.298          3.435          2.586   \n",
       "23         1.235     1.235         1.217          3.414          2.500   \n",
       "24         1.000     1.000         1.000          2.913          2.319   \n",
       "25         2.008     2.008         1.921          3.865          2.920   \n",
       "26         1.607     1.607         1.466          3.183          2.295   \n",
       "27         1.531     1.531         1.420          3.199          2.655   \n",
       "28         1.000     1.000         0.993          2.000          1.000   \n",
       "29         2.473     2.473         2.365          5.368          3.831   \n",
       "30         1.534     1.534         1.439          3.450          2.365   \n",
       "31         2.007     2.007         1.922          3.859          2.862   \n",
       "32         1.603     1.603         1.600          3.732          2.526   \n",
       "33         1.414     1.414         1.397          3.566          2.578   \n",
       "34         2.580     2.580         2.465          5.287          3.449   \n",
       "35         0.826     0.826         0.727          1.555          0.555   \n",
       "36         1.287     1.287         1.305          3.414          2.500   \n",
       "37         1.879     1.879         1.749          2.382          1.000   \n",
       "38         1.516     1.516         1.510          2.724          1.849   \n",
       "39         1.697     1.697         1.728          3.754          2.309   \n",
       "40         2.170     2.170         2.084          4.345          3.115   \n",
       "41         1.868     1.868         1.628          3.578          2.639   \n",
       "\n",
       "    Eig04_AEA(dm)  Eig03_AEA(ri)  Eig04_AEA(ri)  VaporPressure  \n",
       "0          -0.366          1.134          0.344       0.931659  \n",
       "1          -0.659          0.867         -0.105       0.918682  \n",
       "2          -1.328          0.257         -0.697       0.990197  \n",
       "3          -0.596          0.543         -0.099       0.860331  \n",
       "4           1.610          2.634          2.414       0.018548  \n",
       "5           0.000         -0.492          0.000       0.860237  \n",
       "6           1.319          2.403          2.000       0.126692  \n",
       "7           2.286          3.056          1.904       0.008832  \n",
       "8          -0.226          0.000         -0.277       0.897022  \n",
       "9           1.311          2.392          2.005       0.026860  \n",
       "10          1.767          2.151          1.618       0.014070  \n",
       "11          1.562          2.548          2.450       0.017207  \n",
       "12          2.176          3.058          2.521       0.005477  \n",
       "13          1.932          2.309          2.202       0.005477  \n",
       "14          0.739          2.371          1.065       0.782280  \n",
       "15          1.769          3.552          2.566       0.054617  \n",
       "16          1.193          2.315          1.941       0.149258  \n",
       "17          1.652          2.620          1.678       0.071142  \n",
       "18          0.210          2.164          0.577       0.773411  \n",
       "19         -0.986          0.271         -0.662       1.000000  \n",
       "20          2.175          2.975          2.420       0.018514  \n",
       "21          1.595          2.556          2.006       0.024592  \n",
       "22          1.182          2.307          2.000       0.469144  \n",
       "23          1.000          2.238          2.000       0.024181  \n",
       "24          1.065          2.000          1.735       0.296144  \n",
       "25          1.745          2.974          2.490       0.011726  \n",
       "26          1.881          2.524          1.939       0.000331  \n",
       "27          1.432          2.464          2.096       0.192699  \n",
       "28          0.821          2.007          0.762       0.016460  \n",
       "29          2.710          3.432          3.415       0.440949  \n",
       "30          1.986          2.477          2.125       0.113715  \n",
       "31          1.704          2.975          2.392       0.085426  \n",
       "32          1.608          2.602          2.437       0.073476  \n",
       "33          1.461          2.403          2.097       0.149846  \n",
       "34          2.829          3.537          2.947       0.015246  \n",
       "35          0.103          1.715          0.409       0.760993  \n",
       "36          1.067          2.313          2.000       0.290356  \n",
       "37         -0.047          2.816          0.588       0.054897  \n",
       "38          1.203          2.519          1.719       0.009318  \n",
       "39          1.572          2.717          2.053       0.006442  \n",
       "40          2.701          3.145          2.843       0.019907  \n",
       "41          2.319          2.748          2.454       0.020866  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"/Users/philipsmith/Documents/highlycorrelateddescriptors_vp.csv\",sep=\",\")\n",
    "df\n",
    "#header = next(training_file)\n",
    "#delimiter = \",\"\n",
    "#skip_header = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_columns=df.columns\n",
    "x = df[x_columns].values\n",
    "y = df['VaporPressure'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 33 samples, validate on 9 samples\n",
      "Epoch 1/100\n",
      "33/33 - 0s - loss: 0.9717 - val_loss: 0.6476\n",
      "Epoch 2/100\n",
      "33/33 - 0s - loss: 0.8415 - val_loss: 0.6106\n",
      "Epoch 3/100\n",
      "33/33 - 0s - loss: 0.9022 - val_loss: 0.4594\n",
      "Epoch 4/100\n",
      "33/33 - 0s - loss: 0.6438 - val_loss: 0.3496\n",
      "Epoch 5/100\n",
      "33/33 - 0s - loss: 0.7464 - val_loss: 0.3079\n",
      "Epoch 6/100\n",
      "33/33 - 0s - loss: 0.7893 - val_loss: 0.3004\n",
      "Epoch 7/100\n",
      "33/33 - 0s - loss: 0.5462 - val_loss: 0.3281\n",
      "Epoch 8/100\n",
      "33/33 - 0s - loss: 0.7005 - val_loss: 0.3771\n",
      "Epoch 9/100\n",
      "33/33 - 0s - loss: 0.6152 - val_loss: 0.3858\n",
      "Epoch 10/100\n",
      "33/33 - 0s - loss: 0.5871 - val_loss: 0.3636\n",
      "Epoch 11/100\n",
      "33/33 - 0s - loss: 0.4861 - val_loss: 0.3328\n",
      "Epoch 12/100\n",
      "33/33 - 0s - loss: 0.6860 - val_loss: 0.2816\n",
      "Epoch 13/100\n",
      "33/33 - 0s - loss: 0.5652 - val_loss: 0.2498\n",
      "Epoch 14/100\n",
      "33/33 - 0s - loss: 0.6019 - val_loss: 0.2408\n",
      "Epoch 15/100\n",
      "33/33 - 0s - loss: 0.6843 - val_loss: 0.2305\n",
      "Epoch 16/100\n",
      "33/33 - 0s - loss: 0.5847 - val_loss: 0.2203\n",
      "Epoch 17/100\n",
      "33/33 - 0s - loss: 0.5986 - val_loss: 0.2166\n",
      "Epoch 18/100\n",
      "33/33 - 0s - loss: 0.5436 - val_loss: 0.2144\n",
      "Epoch 19/100\n",
      "33/33 - 0s - loss: 0.4899 - val_loss: 0.2103\n",
      "Epoch 20/100\n",
      "33/33 - 0s - loss: 0.4228 - val_loss: 0.2027\n",
      "Epoch 21/100\n",
      "33/33 - 0s - loss: 0.5303 - val_loss: 0.1992\n",
      "Epoch 22/100\n",
      "33/33 - 0s - loss: 0.4406 - val_loss: 0.2031\n",
      "Epoch 23/100\n",
      "33/33 - 0s - loss: 0.5403 - val_loss: 0.2101\n",
      "Epoch 24/100\n",
      "33/33 - 0s - loss: 0.4340 - val_loss: 0.1958\n",
      "Epoch 25/100\n",
      "33/33 - 0s - loss: 0.3871 - val_loss: 0.1894\n",
      "Epoch 26/100\n",
      "33/33 - 0s - loss: 0.4338 - val_loss: 0.1896\n",
      "Epoch 27/100\n",
      "33/33 - 0s - loss: 0.5244 - val_loss: 0.2003\n",
      "Epoch 28/100\n",
      "33/33 - 0s - loss: 0.3551 - val_loss: 0.2111\n",
      "Epoch 29/100\n",
      "33/33 - 0s - loss: 0.5345 - val_loss: 0.1987\n",
      "Epoch 30/100\n",
      "33/33 - 0s - loss: 0.4288 - val_loss: 0.1689\n",
      "Epoch 31/100\n",
      "33/33 - 0s - loss: 0.4414 - val_loss: 0.1487\n",
      "Epoch 32/100\n",
      "33/33 - 0s - loss: 0.4019 - val_loss: 0.1409\n",
      "Epoch 33/100\n",
      "33/33 - 0s - loss: 0.3219 - val_loss: 0.1529\n",
      "Epoch 34/100\n",
      "33/33 - 0s - loss: 0.3864 - val_loss: 0.1685\n",
      "Epoch 35/100\n",
      "33/33 - 0s - loss: 0.4319 - val_loss: 0.1731\n",
      "Epoch 36/100\n",
      "33/33 - 0s - loss: 0.3488 - val_loss: 0.1633\n",
      "Epoch 37/100\n",
      "33/33 - 0s - loss: 0.3536 - val_loss: 0.1675\n",
      "Epoch 38/100\n",
      "33/33 - 0s - loss: 0.4008 - val_loss: 0.1779\n",
      "Epoch 39/100\n",
      "33/33 - 0s - loss: 0.3167 - val_loss: 0.1956\n",
      "Epoch 40/100\n",
      "33/33 - 0s - loss: 0.2845 - val_loss: 0.2144\n",
      "Epoch 41/100\n",
      "33/33 - 0s - loss: 0.3298 - val_loss: 0.2202\n",
      "Epoch 42/100\n",
      "33/33 - 0s - loss: 0.3462 - val_loss: 0.2127\n",
      "Epoch 43/100\n",
      "33/33 - 0s - loss: 0.2949 - val_loss: 0.1971\n",
      "Epoch 44/100\n",
      "33/33 - 0s - loss: 0.2776 - val_loss: 0.1817\n",
      "Epoch 45/100\n",
      "33/33 - 0s - loss: 0.3457 - val_loss: 0.1799\n",
      "Epoch 46/100\n",
      "33/33 - 0s - loss: 0.2736 - val_loss: 0.1760\n",
      "Epoch 47/100\n",
      "33/33 - 0s - loss: 0.3222 - val_loss: 0.1556\n",
      "Epoch 48/100\n",
      "33/33 - 0s - loss: 0.3404 - val_loss: 0.1299\n",
      "Epoch 49/100\n",
      "33/33 - 0s - loss: 0.3056 - val_loss: 0.1212\n",
      "Epoch 50/100\n",
      "33/33 - 0s - loss: 0.3168 - val_loss: 0.1214\n",
      "Epoch 51/100\n",
      "33/33 - 0s - loss: 0.3093 - val_loss: 0.1214\n",
      "Epoch 52/100\n",
      "33/33 - 0s - loss: 0.2799 - val_loss: 0.1105\n",
      "Epoch 53/100\n",
      "33/33 - 0s - loss: 0.3446 - val_loss: 0.1176\n",
      "Epoch 54/100\n",
      "33/33 - 0s - loss: 0.2362 - val_loss: 0.1178\n",
      "Epoch 55/100\n",
      "33/33 - 0s - loss: 0.3550 - val_loss: 0.1134\n",
      "Epoch 56/100\n",
      "33/33 - 0s - loss: 0.2339 - val_loss: 0.1065\n",
      "Epoch 57/100\n",
      "33/33 - 0s - loss: 0.3166 - val_loss: 0.1085\n",
      "Epoch 58/100\n",
      "33/33 - 0s - loss: 0.2807 - val_loss: 0.1159\n",
      "Epoch 59/100\n",
      "33/33 - 0s - loss: 0.2906 - val_loss: 0.1167\n",
      "Epoch 60/100\n",
      "33/33 - 0s - loss: 0.2871 - val_loss: 0.1096\n",
      "Epoch 61/100\n",
      "33/33 - 0s - loss: 0.2690 - val_loss: 0.1043\n",
      "Epoch 62/100\n",
      "33/33 - 0s - loss: 0.2807 - val_loss: 0.1038\n",
      "Epoch 63/100\n",
      "33/33 - 0s - loss: 0.2500 - val_loss: 0.1114\n",
      "Epoch 64/100\n",
      "33/33 - 0s - loss: 0.2465 - val_loss: 0.1339\n",
      "Epoch 65/100\n",
      "33/33 - 0s - loss: 0.3107 - val_loss: 0.1643\n",
      "Epoch 66/100\n",
      "33/33 - 0s - loss: 0.2554 - val_loss: 0.1691\n",
      "Epoch 67/100\n",
      "33/33 - 0s - loss: 0.2516 - val_loss: 0.1434\n",
      "Epoch 68/100\n",
      "33/33 - 0s - loss: 0.2079 - val_loss: 0.1125\n",
      "Epoch 69/100\n",
      "33/33 - 0s - loss: 0.2689 - val_loss: 0.0990\n",
      "Epoch 70/100\n",
      "33/33 - 0s - loss: 0.1997 - val_loss: 0.0969\n",
      "Epoch 71/100\n",
      "33/33 - 0s - loss: 0.2617 - val_loss: 0.0962\n",
      "Epoch 72/100\n",
      "33/33 - 0s - loss: 0.2378 - val_loss: 0.0942\n",
      "Epoch 73/100\n",
      "33/33 - 0s - loss: 0.2443 - val_loss: 0.0960\n",
      "Epoch 74/100\n",
      "33/33 - 0s - loss: 0.2386 - val_loss: 0.1222\n",
      "Epoch 75/100\n",
      "33/33 - 0s - loss: 0.2065 - val_loss: 0.1662\n",
      "Epoch 76/100\n",
      "33/33 - 0s - loss: 0.3040 - val_loss: 0.1972\n",
      "Epoch 77/100\n",
      "33/33 - 0s - loss: 0.2109 - val_loss: 0.1940\n",
      "Epoch 78/100\n",
      "33/33 - 0s - loss: 0.2817 - val_loss: 0.1624\n",
      "Epoch 79/100\n",
      "33/33 - 0s - loss: 0.3196 - val_loss: 0.1209\n",
      "Epoch 80/100\n",
      "33/33 - 0s - loss: 0.2209 - val_loss: 0.1044\n",
      "Epoch 81/100\n",
      "33/33 - 0s - loss: 0.2853 - val_loss: 0.1389\n",
      "Epoch 82/100\n",
      "33/33 - 0s - loss: 0.2424 - val_loss: 0.1997\n",
      "Epoch 83/100\n",
      "33/33 - 0s - loss: 0.2389 - val_loss: 0.2405\n",
      "Epoch 84/100\n",
      "33/33 - 0s - loss: 0.2036 - val_loss: 0.2760\n",
      "Epoch 85/100\n",
      "33/33 - 0s - loss: 0.3547 - val_loss: 0.3199\n",
      "Epoch 86/100\n",
      "33/33 - 0s - loss: 0.3408 - val_loss: 0.3410\n",
      "Epoch 87/100\n",
      "33/33 - 0s - loss: 0.3308 - val_loss: 0.3477\n",
      "Epoch 88/100\n",
      "33/33 - 0s - loss: 0.4164 - val_loss: 0.3223\n",
      "Epoch 89/100\n",
      "33/33 - 0s - loss: 0.3856 - val_loss: 0.2481\n",
      "Epoch 90/100\n",
      "33/33 - 0s - loss: 0.3304 - val_loss: 0.1731\n",
      "Epoch 91/100\n",
      "33/33 - 0s - loss: 0.2953 - val_loss: 0.1194\n",
      "Epoch 92/100\n",
      "33/33 - 0s - loss: 0.1933 - val_loss: 0.1031\n",
      "Epoch 93/100\n",
      "33/33 - 0s - loss: 0.2180 - val_loss: 0.1276\n",
      "Epoch 94/100\n",
      "33/33 - 0s - loss: 0.2800 - val_loss: 0.1409\n",
      "Epoch 95/100\n",
      "33/33 - 0s - loss: 0.1949 - val_loss: 0.1345\n",
      "Epoch 96/100\n",
      "33/33 - 0s - loss: 0.1860 - val_loss: 0.1135\n",
      "Epoch 97/100\n",
      "33/33 - 0s - loss: 0.2148 - val_loss: 0.0927\n",
      "Epoch 98/100\n",
      "33/33 - 0s - loss: 0.2767 - val_loss: 0.1015\n",
      "Epoch 99/100\n",
      "33/33 - 0s - loss: 0.2873 - val_loss: 0.1167\n",
      "Epoch 100/100\n",
      "33/33 - 0s - loss: 0.2662 - val_loss: 0.1418\n",
      "Fold score (MAE): 0.37660955399322665\n",
      "Fold #2\n",
      "Train on 33 samples, validate on 9 samples\n",
      "Epoch 1/100\n",
      "33/33 - 0s - loss: 2.8123 - val_loss: 2.0914\n",
      "Epoch 2/100\n",
      "33/33 - 0s - loss: 2.4303 - val_loss: 1.6844\n",
      "Epoch 3/100\n",
      "33/33 - 0s - loss: 2.0872 - val_loss: 1.3201\n",
      "Epoch 4/100\n",
      "33/33 - 0s - loss: 1.8323 - val_loss: 0.9854\n",
      "Epoch 5/100\n",
      "33/33 - 0s - loss: 1.3686 - val_loss: 0.6892\n",
      "Epoch 6/100\n",
      "33/33 - 0s - loss: 1.1422 - val_loss: 0.4235\n",
      "Epoch 7/100\n",
      "33/33 - 0s - loss: 1.1959 - val_loss: 0.1623\n",
      "Epoch 8/100\n",
      "33/33 - 0s - loss: 0.9167 - val_loss: 0.0945\n",
      "Epoch 9/100\n",
      "33/33 - 0s - loss: 0.6163 - val_loss: 0.3092\n",
      "Epoch 10/100\n",
      "33/33 - 0s - loss: 0.5782 - val_loss: 0.5019\n",
      "Epoch 11/100\n",
      "33/33 - 0s - loss: 0.6162 - val_loss: 0.6714\n",
      "Epoch 12/100\n",
      "33/33 - 0s - loss: 0.6713 - val_loss: 0.7984\n",
      "Epoch 13/100\n",
      "33/33 - 0s - loss: 0.7271 - val_loss: 0.8715\n",
      "Epoch 14/100\n",
      "33/33 - 0s - loss: 0.8043 - val_loss: 0.8851\n",
      "Epoch 15/100\n",
      "33/33 - 0s - loss: 0.5729 - val_loss: 0.9000\n",
      "Epoch 16/100\n",
      "33/33 - 0s - loss: 0.7061 - val_loss: 0.8849\n",
      "Epoch 17/100\n",
      "33/33 - 0s - loss: 0.8058 - val_loss: 0.8134\n",
      "Epoch 18/100\n",
      "33/33 - 0s - loss: 0.7027 - val_loss: 0.7132\n",
      "Epoch 19/100\n",
      "33/33 - 0s - loss: 0.5674 - val_loss: 0.6195\n",
      "Epoch 20/100\n",
      "33/33 - 0s - loss: 0.5713 - val_loss: 0.5249\n",
      "Epoch 21/100\n",
      "33/33 - 0s - loss: 0.5823 - val_loss: 0.4460\n",
      "Epoch 22/100\n",
      "33/33 - 0s - loss: 0.4794 - val_loss: 0.3630\n",
      "Epoch 23/100\n",
      "33/33 - 0s - loss: 0.4031 - val_loss: 0.2820\n",
      "Epoch 24/100\n",
      "33/33 - 0s - loss: 0.3714 - val_loss: 0.2070\n",
      "Epoch 25/100\n",
      "33/33 - 0s - loss: 0.3541 - val_loss: 0.1396\n",
      "Epoch 26/100\n",
      "33/33 - 0s - loss: 0.3484 - val_loss: 0.1153\n",
      "Epoch 27/100\n",
      "33/33 - 0s - loss: 0.4338 - val_loss: 0.1415\n",
      "Epoch 28/100\n",
      "33/33 - 0s - loss: 0.3565 - val_loss: 0.1873\n",
      "Epoch 29/100\n",
      "33/33 - 0s - loss: 0.3628 - val_loss: 0.2274\n",
      "Epoch 30/100\n",
      "33/33 - 0s - loss: 0.4388 - val_loss: 0.2628\n",
      "Epoch 31/100\n",
      "33/33 - 0s - loss: 0.4354 - val_loss: 0.3035\n",
      "Epoch 32/100\n",
      "33/33 - 0s - loss: 0.3417 - val_loss: 0.3558\n",
      "Epoch 33/100\n",
      "33/33 - 0s - loss: 0.4830 - val_loss: 0.3919\n",
      "Epoch 34/100\n",
      "33/33 - 0s - loss: 0.3829 - val_loss: 0.3898\n",
      "Epoch 35/100\n",
      "33/33 - 0s - loss: 0.4154 - val_loss: 0.3822\n",
      "Epoch 36/100\n",
      "33/33 - 0s - loss: 0.4949 - val_loss: 0.3926\n",
      "Epoch 37/100\n",
      "33/33 - 0s - loss: 0.5128 - val_loss: 0.3905\n",
      "Epoch 38/100\n",
      "33/33 - 0s - loss: 0.3910 - val_loss: 0.3788\n",
      "Epoch 39/100\n",
      "33/33 - 0s - loss: 0.4181 - val_loss: 0.3540\n",
      "Epoch 40/100\n",
      "33/33 - 0s - loss: 0.3390 - val_loss: 0.3105\n",
      "Epoch 41/100\n",
      "33/33 - 0s - loss: 0.2720 - val_loss: 0.2427\n",
      "Epoch 42/100\n",
      "33/33 - 0s - loss: 0.3528 - val_loss: 0.1496\n",
      "Epoch 43/100\n",
      "33/33 - 0s - loss: 0.3420 - val_loss: 0.0799\n",
      "Epoch 44/100\n",
      "33/33 - 0s - loss: 0.3535 - val_loss: 0.0549\n",
      "Epoch 45/100\n",
      "33/33 - 0s - loss: 0.3705 - val_loss: 0.0493\n",
      "Epoch 46/100\n",
      "33/33 - 0s - loss: 0.4031 - val_loss: 0.0580\n",
      "Epoch 47/100\n",
      "33/33 - 0s - loss: 0.4413 - val_loss: 0.0831\n",
      "Epoch 48/100\n",
      "33/33 - 0s - loss: 0.3701 - val_loss: 0.0940\n",
      "Epoch 49/100\n",
      "33/33 - 0s - loss: 0.3275 - val_loss: 0.0795\n",
      "Epoch 50/100\n",
      "33/33 - 0s - loss: 0.2668 - val_loss: 0.0641\n",
      "Epoch 51/100\n",
      "33/33 - 0s - loss: 0.2710 - val_loss: 0.0680\n",
      "Epoch 52/100\n",
      "33/33 - 0s - loss: 0.2475 - val_loss: 0.0823\n",
      "Epoch 53/100\n",
      "33/33 - 0s - loss: 0.2914 - val_loss: 0.1006\n",
      "Epoch 54/100\n",
      "33/33 - 0s - loss: 0.2851 - val_loss: 0.1314\n",
      "Epoch 55/100\n",
      "33/33 - 0s - loss: 0.2933 - val_loss: 0.1516\n",
      "Epoch 56/100\n",
      "33/33 - 0s - loss: 0.2876 - val_loss: 0.1482\n",
      "Epoch 57/100\n",
      "33/33 - 0s - loss: 0.3971 - val_loss: 0.1467\n",
      "Epoch 58/100\n",
      "33/33 - 0s - loss: 0.3711 - val_loss: 0.1475\n",
      "Epoch 59/100\n",
      "33/33 - 0s - loss: 0.3380 - val_loss: 0.1675\n",
      "Epoch 60/100\n",
      "33/33 - 0s - loss: 0.2949 - val_loss: 0.1996\n",
      "Epoch 61/100\n",
      "33/33 - 0s - loss: 0.2712 - val_loss: 0.2254\n",
      "Epoch 62/100\n",
      "33/33 - 0s - loss: 0.2833 - val_loss: 0.2141\n",
      "Epoch 63/100\n",
      "33/33 - 0s - loss: 0.3304 - val_loss: 0.1673\n",
      "Epoch 64/100\n",
      "33/33 - 0s - loss: 0.2529 - val_loss: 0.0966\n",
      "Epoch 65/100\n",
      "33/33 - 0s - loss: 0.3142 - val_loss: 0.0402\n",
      "Epoch 66/100\n",
      "33/33 - 0s - loss: 0.2441 - val_loss: 0.0808\n",
      "Epoch 67/100\n",
      "33/33 - 0s - loss: 0.2992 - val_loss: 0.1322\n",
      "Epoch 68/100\n",
      "33/33 - 0s - loss: 0.2754 - val_loss: 0.1681\n",
      "Epoch 69/100\n",
      "33/33 - 0s - loss: 0.3239 - val_loss: 0.1961\n",
      "Epoch 70/100\n",
      "33/33 - 0s - loss: 0.3474 - val_loss: 0.2429\n",
      "Epoch 71/100\n",
      "33/33 - 0s - loss: 0.3445 - val_loss: 0.2952\n",
      "Epoch 72/100\n",
      "33/33 - 0s - loss: 0.3316 - val_loss: 0.3183\n",
      "Epoch 73/100\n",
      "33/33 - 0s - loss: 0.3507 - val_loss: 0.2942\n",
      "Epoch 74/100\n",
      "33/33 - 0s - loss: 0.3108 - val_loss: 0.2353\n",
      "Epoch 75/100\n",
      "33/33 - 0s - loss: 0.3510 - val_loss: 0.1458\n",
      "Epoch 76/100\n",
      "33/33 - 0s - loss: 0.2841 - val_loss: 0.0576\n",
      "Epoch 77/100\n",
      "33/33 - 0s - loss: 0.2615 - val_loss: 0.0442\n",
      "Epoch 78/100\n",
      "33/33 - 0s - loss: 0.2302 - val_loss: 0.0811\n",
      "Epoch 79/100\n",
      "33/33 - 0s - loss: 0.2922 - val_loss: 0.1434\n",
      "Epoch 80/100\n",
      "33/33 - 0s - loss: 0.3036 - val_loss: 0.1904\n",
      "Epoch 81/100\n",
      "33/33 - 0s - loss: 0.2261 - val_loss: 0.1915\n",
      "Epoch 82/100\n",
      "33/33 - 0s - loss: 0.3052 - val_loss: 0.1861\n",
      "Epoch 83/100\n",
      "33/33 - 0s - loss: 0.2714 - val_loss: 0.1800\n",
      "Epoch 84/100\n",
      "33/33 - 0s - loss: 0.2593 - val_loss: 0.1507\n",
      "Epoch 85/100\n",
      "33/33 - 0s - loss: 0.3014 - val_loss: 0.0948\n",
      "Epoch 86/100\n",
      "33/33 - 0s - loss: 0.2449 - val_loss: 0.0476\n",
      "Epoch 87/100\n",
      "33/33 - 0s - loss: 0.2051 - val_loss: 0.0506\n",
      "Epoch 88/100\n",
      "33/33 - 0s - loss: 0.1924 - val_loss: 0.0938\n",
      "Epoch 89/100\n",
      "33/33 - 0s - loss: 0.2471 - val_loss: 0.1565\n",
      "Epoch 90/100\n",
      "33/33 - 0s - loss: 0.2275 - val_loss: 0.2236\n",
      "Epoch 91/100\n",
      "33/33 - 0s - loss: 0.2789 - val_loss: 0.2728\n",
      "Epoch 92/100\n",
      "33/33 - 0s - loss: 0.3348 - val_loss: 0.2868\n",
      "Epoch 93/100\n",
      "33/33 - 0s - loss: 0.3687 - val_loss: 0.2632\n",
      "Epoch 94/100\n",
      "33/33 - 0s - loss: 0.3205 - val_loss: 0.2316\n",
      "Epoch 95/100\n",
      "33/33 - 0s - loss: 0.3081 - val_loss: 0.2019\n",
      "Epoch 96/100\n",
      "33/33 - 0s - loss: 0.2561 - val_loss: 0.1571\n",
      "Epoch 97/100\n",
      "33/33 - 0s - loss: 0.2518 - val_loss: 0.0937\n",
      "Epoch 98/100\n",
      "33/33 - 0s - loss: 0.2592 - val_loss: 0.0589\n",
      "Epoch 99/100\n",
      "33/33 - 0s - loss: 0.2651 - val_loss: 0.0631\n",
      "Epoch 100/100\n",
      "33/33 - 0s - loss: 0.2548 - val_loss: 0.0982\n",
      "Fold score (MAE): 0.3134177951790547\n",
      "Fold #3\n",
      "Train on 34 samples, validate on 8 samples\n",
      "Epoch 1/100\n",
      "34/34 - 0s - loss: 0.5101 - val_loss: 0.2072\n",
      "Epoch 2/100\n",
      "34/34 - 0s - loss: 0.6937 - val_loss: 0.1990\n",
      "Epoch 3/100\n",
      "34/34 - 0s - loss: 0.6471 - val_loss: 0.2025\n",
      "Epoch 4/100\n",
      "34/34 - 0s - loss: 0.5816 - val_loss: 0.1971\n",
      "Epoch 5/100\n",
      "34/34 - 0s - loss: 0.5401 - val_loss: 0.1969\n",
      "Epoch 6/100\n",
      "34/34 - 0s - loss: 0.5760 - val_loss: 0.2245\n",
      "Epoch 7/100\n",
      "34/34 - 0s - loss: 0.5374 - val_loss: 0.2546\n",
      "Epoch 8/100\n",
      "34/34 - 0s - loss: 0.6392 - val_loss: 0.2514\n",
      "Epoch 9/100\n",
      "34/34 - 0s - loss: 0.4837 - val_loss: 0.2354\n",
      "Epoch 10/100\n",
      "34/34 - 0s - loss: 0.5421 - val_loss: 0.1885\n",
      "Epoch 11/100\n",
      "34/34 - 0s - loss: 0.5409 - val_loss: 0.1731\n",
      "Epoch 12/100\n",
      "34/34 - 0s - loss: 0.5831 - val_loss: 0.1653\n",
      "Epoch 13/100\n",
      "34/34 - 0s - loss: 0.4354 - val_loss: 0.1558\n",
      "Epoch 14/100\n",
      "34/34 - 0s - loss: 0.4132 - val_loss: 0.1711\n",
      "Epoch 15/100\n",
      "34/34 - 0s - loss: 0.4311 - val_loss: 0.2198\n",
      "Epoch 16/100\n",
      "34/34 - 0s - loss: 0.4543 - val_loss: 0.2518\n",
      "Epoch 17/100\n",
      "34/34 - 0s - loss: 0.5055 - val_loss: 0.2824\n",
      "Epoch 18/100\n",
      "34/34 - 0s - loss: 0.3582 - val_loss: 0.2990\n",
      "Epoch 19/100\n",
      "34/34 - 0s - loss: 0.3335 - val_loss: 0.2948\n",
      "Epoch 20/100\n",
      "34/34 - 0s - loss: 0.3708 - val_loss: 0.3132\n",
      "Epoch 21/100\n",
      "34/34 - 0s - loss: 0.5348 - val_loss: 0.2931\n",
      "Epoch 22/100\n",
      "34/34 - 0s - loss: 0.4317 - val_loss: 0.2500\n",
      "Epoch 23/100\n",
      "34/34 - 0s - loss: 0.3879 - val_loss: 0.2086\n",
      "Epoch 24/100\n",
      "34/34 - 0s - loss: 0.3314 - val_loss: 0.1781\n",
      "Epoch 25/100\n",
      "34/34 - 0s - loss: 0.3159 - val_loss: 0.1624\n",
      "Epoch 26/100\n",
      "34/34 - 0s - loss: 0.3854 - val_loss: 0.1625\n",
      "Epoch 27/100\n",
      "34/34 - 0s - loss: 0.3395 - val_loss: 0.1671\n",
      "Epoch 28/100\n",
      "34/34 - 0s - loss: 0.2983 - val_loss: 0.1801\n",
      "Epoch 29/100\n",
      "34/34 - 0s - loss: 0.3785 - val_loss: 0.1683\n",
      "Epoch 30/100\n",
      "34/34 - 0s - loss: 0.4090 - val_loss: 0.1638\n",
      "Epoch 31/100\n",
      "34/34 - 0s - loss: 0.3758 - val_loss: 0.1782\n",
      "Epoch 32/100\n",
      "34/34 - 0s - loss: 0.3900 - val_loss: 0.1947\n",
      "Epoch 33/100\n",
      "34/34 - 0s - loss: 0.4130 - val_loss: 0.1920\n",
      "Epoch 34/100\n",
      "34/34 - 0s - loss: 0.3774 - val_loss: 0.1836\n",
      "Epoch 35/100\n",
      "34/34 - 0s - loss: 0.2667 - val_loss: 0.1993\n",
      "Epoch 36/100\n",
      "34/34 - 0s - loss: 0.3483 - val_loss: 0.2483\n",
      "Epoch 37/100\n",
      "34/34 - 0s - loss: 0.3164 - val_loss: 0.3043\n",
      "Epoch 38/100\n",
      "34/34 - 0s - loss: 0.3971 - val_loss: 0.3382\n",
      "Epoch 39/100\n",
      "34/34 - 0s - loss: 0.4592 - val_loss: 0.3077\n",
      "Epoch 40/100\n",
      "34/34 - 0s - loss: 0.3087 - val_loss: 0.2379\n",
      "Epoch 41/100\n",
      "34/34 - 0s - loss: 0.3209 - val_loss: 0.1995\n",
      "Epoch 42/100\n",
      "34/34 - 0s - loss: 0.3235 - val_loss: 0.2248\n",
      "Epoch 43/100\n",
      "34/34 - 0s - loss: 0.3916 - val_loss: 0.2517\n",
      "Epoch 44/100\n",
      "34/34 - 0s - loss: 0.3147 - val_loss: 0.2519\n",
      "Epoch 45/100\n",
      "34/34 - 0s - loss: 0.3836 - val_loss: 0.2279\n",
      "Epoch 46/100\n",
      "34/34 - 0s - loss: 0.2733 - val_loss: 0.2085\n",
      "Epoch 47/100\n",
      "34/34 - 0s - loss: 0.2991 - val_loss: 0.2097\n",
      "Epoch 48/100\n",
      "34/34 - 0s - loss: 0.3587 - val_loss: 0.2201\n",
      "Epoch 49/100\n",
      "34/34 - 0s - loss: 0.2445 - val_loss: 0.2252\n",
      "Epoch 50/100\n",
      "34/34 - 0s - loss: 0.3013 - val_loss: 0.2311\n",
      "Epoch 51/100\n",
      "34/34 - 0s - loss: 0.2595 - val_loss: 0.2323\n",
      "Epoch 52/100\n",
      "34/34 - 0s - loss: 0.2295 - val_loss: 0.2177\n",
      "Epoch 53/100\n",
      "34/34 - 0s - loss: 0.2695 - val_loss: 0.2257\n",
      "Epoch 54/100\n",
      "34/34 - 0s - loss: 0.2756 - val_loss: 0.2453\n",
      "Epoch 55/100\n",
      "34/34 - 0s - loss: 0.2919 - val_loss: 0.2657\n",
      "Epoch 56/100\n",
      "34/34 - 0s - loss: 0.3096 - val_loss: 0.2759\n",
      "Epoch 57/100\n",
      "34/34 - 0s - loss: 0.2844 - val_loss: 0.2634\n",
      "Epoch 58/100\n",
      "34/34 - 0s - loss: 0.2343 - val_loss: 0.2432\n",
      "Epoch 59/100\n",
      "34/34 - 0s - loss: 0.2716 - val_loss: 0.2349\n",
      "Epoch 60/100\n",
      "34/34 - 0s - loss: 0.2825 - val_loss: 0.2272\n",
      "Epoch 61/100\n",
      "34/34 - 0s - loss: 0.2756 - val_loss: 0.2172\n",
      "Epoch 62/100\n",
      "34/34 - 0s - loss: 0.2915 - val_loss: 0.2105\n",
      "Epoch 63/100\n",
      "34/34 - 0s - loss: 0.2601 - val_loss: 0.2168\n",
      "Epoch 64/100\n",
      "34/34 - 0s - loss: 0.2095 - val_loss: 0.2284\n",
      "Epoch 65/100\n",
      "34/34 - 0s - loss: 0.3039 - val_loss: 0.2314\n",
      "Epoch 66/100\n",
      "34/34 - 0s - loss: 0.3021 - val_loss: 0.2126\n",
      "Epoch 67/100\n",
      "34/34 - 0s - loss: 0.2256 - val_loss: 0.2026\n",
      "Epoch 68/100\n",
      "34/34 - 0s - loss: 0.2293 - val_loss: 0.2081\n",
      "Epoch 69/100\n",
      "34/34 - 0s - loss: 0.3014 - val_loss: 0.2189\n",
      "Epoch 70/100\n",
      "34/34 - 0s - loss: 0.2944 - val_loss: 0.2272\n",
      "Epoch 71/100\n",
      "34/34 - 0s - loss: 0.2707 - val_loss: 0.2344\n",
      "Epoch 72/100\n",
      "34/34 - 0s - loss: 0.1909 - val_loss: 0.2353\n",
      "Epoch 73/100\n",
      "34/34 - 0s - loss: 0.3019 - val_loss: 0.2365\n",
      "Epoch 74/100\n",
      "34/34 - 0s - loss: 0.3046 - val_loss: 0.2437\n",
      "Epoch 75/100\n",
      "34/34 - 0s - loss: 0.2391 - val_loss: 0.2531\n",
      "Epoch 76/100\n",
      "34/34 - 0s - loss: 0.2592 - val_loss: 0.2645\n",
      "Epoch 77/100\n",
      "34/34 - 0s - loss: 0.2466 - val_loss: 0.2723\n",
      "Epoch 78/100\n",
      "34/34 - 0s - loss: 0.2259 - val_loss: 0.2716\n",
      "Epoch 79/100\n",
      "34/34 - 0s - loss: 0.2118 - val_loss: 0.2644\n",
      "Epoch 80/100\n",
      "34/34 - 0s - loss: 0.2695 - val_loss: 0.2516\n",
      "Epoch 81/100\n",
      "34/34 - 0s - loss: 0.2427 - val_loss: 0.2464\n",
      "Epoch 82/100\n",
      "34/34 - 0s - loss: 0.2075 - val_loss: 0.2453\n",
      "Epoch 83/100\n",
      "34/34 - 0s - loss: 0.2251 - val_loss: 0.2393\n",
      "Epoch 84/100\n",
      "34/34 - 0s - loss: 0.2499 - val_loss: 0.2404\n",
      "Epoch 85/100\n",
      "34/34 - 0s - loss: 0.2400 - val_loss: 0.2419\n",
      "Epoch 86/100\n",
      "34/34 - 0s - loss: 0.2153 - val_loss: 0.2406\n",
      "Epoch 87/100\n",
      "34/34 - 0s - loss: 0.2338 - val_loss: 0.2319\n",
      "Epoch 88/100\n",
      "34/34 - 0s - loss: 0.2022 - val_loss: 0.2235\n",
      "Epoch 89/100\n",
      "34/34 - 0s - loss: 0.2030 - val_loss: 0.2223\n",
      "Epoch 90/100\n",
      "34/34 - 0s - loss: 0.1880 - val_loss: 0.2190\n",
      "Epoch 91/100\n",
      "34/34 - 0s - loss: 0.2067 - val_loss: 0.2136\n",
      "Epoch 92/100\n",
      "34/34 - 0s - loss: 0.2158 - val_loss: 0.2235\n",
      "Epoch 93/100\n",
      "34/34 - 0s - loss: 0.2381 - val_loss: 0.2355\n",
      "Epoch 94/100\n",
      "34/34 - 0s - loss: 0.1927 - val_loss: 0.2484\n",
      "Epoch 95/100\n",
      "34/34 - 0s - loss: 0.2948 - val_loss: 0.2477\n",
      "Epoch 96/100\n",
      "34/34 - 0s - loss: 0.3197 - val_loss: 0.2324\n",
      "Epoch 97/100\n",
      "34/34 - 0s - loss: 0.2311 - val_loss: 0.2246\n",
      "Epoch 98/100\n",
      "34/34 - 0s - loss: 0.1948 - val_loss: 0.2317\n",
      "Epoch 99/100\n",
      "34/34 - 0s - loss: 0.2453 - val_loss: 0.2286\n",
      "Epoch 100/100\n",
      "34/34 - 0s - loss: 0.1996 - val_loss: 0.2220\n",
      "Fold score (MAE): 0.4711621865009644\n",
      "Fold #4\n",
      "Train on 34 samples, validate on 8 samples\n",
      "Epoch 1/100\n",
      "34/34 - 0s - loss: 1.7830 - val_loss: 1.2750\n",
      "Epoch 2/100\n",
      "34/34 - 0s - loss: 1.6063 - val_loss: 0.9273\n",
      "Epoch 3/100\n",
      "34/34 - 0s - loss: 1.0515 - val_loss: 0.6066\n",
      "Epoch 4/100\n",
      "34/34 - 0s - loss: 0.8920 - val_loss: 0.3656\n",
      "Epoch 5/100\n",
      "34/34 - 0s - loss: 0.6133 - val_loss: 0.4481\n",
      "Epoch 6/100\n",
      "34/34 - 0s - loss: 0.5310 - val_loss: 0.6131\n",
      "Epoch 7/100\n",
      "34/34 - 0s - loss: 0.4965 - val_loss: 0.7244\n",
      "Epoch 8/100\n",
      "34/34 - 0s - loss: 0.5889 - val_loss: 0.7890\n",
      "Epoch 9/100\n",
      "34/34 - 0s - loss: 0.5513 - val_loss: 0.8113\n",
      "Epoch 10/100\n",
      "34/34 - 0s - loss: 0.6128 - val_loss: 0.7912\n",
      "Epoch 11/100\n",
      "34/34 - 0s - loss: 0.5774 - val_loss: 0.7434\n",
      "Epoch 12/100\n",
      "34/34 - 0s - loss: 0.5910 - val_loss: 0.6641\n",
      "Epoch 13/100\n",
      "34/34 - 0s - loss: 0.4298 - val_loss: 0.5796\n",
      "Epoch 14/100\n",
      "34/34 - 0s - loss: 0.4818 - val_loss: 0.4942\n",
      "Epoch 15/100\n",
      "34/34 - 0s - loss: 0.4056 - val_loss: 0.4040\n",
      "Epoch 16/100\n",
      "34/34 - 0s - loss: 0.4154 - val_loss: 0.3274\n",
      "Epoch 17/100\n",
      "34/34 - 0s - loss: 0.3947 - val_loss: 0.2613\n",
      "Epoch 18/100\n",
      "34/34 - 0s - loss: 0.4666 - val_loss: 0.2260\n",
      "Epoch 19/100\n",
      "34/34 - 0s - loss: 0.4447 - val_loss: 0.2305\n",
      "Epoch 20/100\n",
      "34/34 - 0s - loss: 0.4740 - val_loss: 0.2350\n",
      "Epoch 21/100\n",
      "34/34 - 0s - loss: 0.4399 - val_loss: 0.2310\n",
      "Epoch 22/100\n",
      "34/34 - 0s - loss: 0.3178 - val_loss: 0.2280\n",
      "Epoch 23/100\n",
      "34/34 - 0s - loss: 0.3146 - val_loss: 0.2389\n",
      "Epoch 24/100\n",
      "34/34 - 0s - loss: 0.3779 - val_loss: 0.2608\n",
      "Epoch 25/100\n",
      "34/34 - 0s - loss: 0.3225 - val_loss: 0.2945\n",
      "Epoch 26/100\n",
      "34/34 - 0s - loss: 0.3248 - val_loss: 0.3226\n",
      "Epoch 27/100\n",
      "34/34 - 0s - loss: 0.4030 - val_loss: 0.3439\n",
      "Epoch 28/100\n",
      "34/34 - 0s - loss: 0.4053 - val_loss: 0.3574\n",
      "Epoch 29/100\n",
      "34/34 - 0s - loss: 0.3117 - val_loss: 0.3678\n",
      "Epoch 30/100\n",
      "34/34 - 0s - loss: 0.3297 - val_loss: 0.3769\n",
      "Epoch 31/100\n",
      "34/34 - 0s - loss: 0.3154 - val_loss: 0.3803\n",
      "Epoch 32/100\n",
      "34/34 - 0s - loss: 0.2862 - val_loss: 0.3674\n",
      "Epoch 33/100\n",
      "34/34 - 0s - loss: 0.2013 - val_loss: 0.3460\n",
      "Epoch 34/100\n",
      "34/34 - 0s - loss: 0.2746 - val_loss: 0.3216\n",
      "Epoch 35/100\n",
      "34/34 - 0s - loss: 0.2683 - val_loss: 0.2825\n",
      "Epoch 36/100\n",
      "34/34 - 0s - loss: 0.2423 - val_loss: 0.2266\n",
      "Epoch 37/100\n",
      "34/34 - 0s - loss: 0.2532 - val_loss: 0.1683\n",
      "Epoch 38/100\n",
      "34/34 - 0s - loss: 0.2473 - val_loss: 0.1166\n",
      "Epoch 39/100\n",
      "34/34 - 0s - loss: 0.2568 - val_loss: 0.1008\n",
      "Epoch 40/100\n",
      "34/34 - 0s - loss: 0.2802 - val_loss: 0.1047\n",
      "Epoch 41/100\n",
      "34/34 - 0s - loss: 0.2496 - val_loss: 0.1187\n",
      "Epoch 42/100\n",
      "34/34 - 0s - loss: 0.2747 - val_loss: 0.1505\n",
      "Epoch 43/100\n",
      "34/34 - 0s - loss: 0.2692 - val_loss: 0.1887\n",
      "Epoch 44/100\n",
      "34/34 - 0s - loss: 0.2097 - val_loss: 0.2132\n",
      "Epoch 45/100\n",
      "34/34 - 0s - loss: 0.2668 - val_loss: 0.2203\n",
      "Epoch 46/100\n",
      "34/34 - 0s - loss: 0.2739 - val_loss: 0.2147\n",
      "Epoch 47/100\n",
      "34/34 - 0s - loss: 0.2360 - val_loss: 0.2089\n",
      "Epoch 48/100\n",
      "34/34 - 0s - loss: 0.2182 - val_loss: 0.1892\n",
      "Epoch 49/100\n",
      "34/34 - 0s - loss: 0.2705 - val_loss: 0.1616\n",
      "Epoch 50/100\n",
      "34/34 - 0s - loss: 0.2228 - val_loss: 0.1481\n",
      "Epoch 51/100\n",
      "34/34 - 0s - loss: 0.2233 - val_loss: 0.1494\n",
      "Epoch 52/100\n",
      "34/34 - 0s - loss: 0.2389 - val_loss: 0.1450\n",
      "Epoch 53/100\n",
      "34/34 - 0s - loss: 0.2190 - val_loss: 0.1430\n",
      "Epoch 54/100\n",
      "34/34 - 0s - loss: 0.1830 - val_loss: 0.1380\n",
      "Epoch 55/100\n",
      "34/34 - 0s - loss: 0.2029 - val_loss: 0.1148\n",
      "Epoch 56/100\n",
      "34/34 - 0s - loss: 0.1976 - val_loss: 0.1015\n",
      "Epoch 57/100\n",
      "34/34 - 0s - loss: 0.2200 - val_loss: 0.1107\n",
      "Epoch 58/100\n",
      "34/34 - 0s - loss: 0.2274 - val_loss: 0.1198\n",
      "Epoch 59/100\n",
      "34/34 - 0s - loss: 0.2176 - val_loss: 0.1257\n",
      "Epoch 60/100\n",
      "34/34 - 0s - loss: 0.2199 - val_loss: 0.1247\n",
      "Epoch 61/100\n",
      "34/34 - 0s - loss: 0.2491 - val_loss: 0.1225\n",
      "Epoch 62/100\n",
      "34/34 - 0s - loss: 0.2360 - val_loss: 0.1253\n",
      "Epoch 63/100\n",
      "34/34 - 0s - loss: 0.2119 - val_loss: 0.1290\n",
      "Epoch 64/100\n",
      "34/34 - 0s - loss: 0.2712 - val_loss: 0.1396\n",
      "Epoch 65/100\n",
      "34/34 - 0s - loss: 0.1989 - val_loss: 0.1554\n",
      "Epoch 66/100\n",
      "34/34 - 0s - loss: 0.2045 - val_loss: 0.1656\n",
      "Epoch 67/100\n",
      "34/34 - 0s - loss: 0.2274 - val_loss: 0.1787\n",
      "Epoch 68/100\n",
      "34/34 - 0s - loss: 0.2069 - val_loss: 0.1949\n",
      "Epoch 69/100\n",
      "34/34 - 0s - loss: 0.1901 - val_loss: 0.2049\n",
      "Epoch 70/100\n",
      "34/34 - 0s - loss: 0.2452 - val_loss: 0.2105\n",
      "Epoch 71/100\n",
      "34/34 - 0s - loss: 0.2381 - val_loss: 0.2076\n",
      "Epoch 72/100\n",
      "34/34 - 0s - loss: 0.1952 - val_loss: 0.2025\n",
      "Epoch 73/100\n",
      "34/34 - 0s - loss: 0.2251 - val_loss: 0.2087\n",
      "Epoch 74/100\n",
      "34/34 - 0s - loss: 0.2628 - val_loss: 0.2128\n",
      "Epoch 75/100\n",
      "34/34 - 0s - loss: 0.2092 - val_loss: 0.2105\n",
      "Epoch 76/100\n",
      "34/34 - 0s - loss: 0.1929 - val_loss: 0.1961\n",
      "Epoch 77/100\n",
      "34/34 - 0s - loss: 0.2245 - val_loss: 0.1663\n",
      "Epoch 78/100\n",
      "34/34 - 0s - loss: 0.1842 - val_loss: 0.1239\n",
      "Epoch 79/100\n",
      "34/34 - 0s - loss: 0.1731 - val_loss: 0.0916\n",
      "Epoch 80/100\n",
      "34/34 - 0s - loss: 0.1969 - val_loss: 0.0954\n",
      "Epoch 81/100\n",
      "34/34 - 0s - loss: 0.1774 - val_loss: 0.1023\n",
      "Epoch 82/100\n",
      "34/34 - 0s - loss: 0.1932 - val_loss: 0.0984\n",
      "Epoch 83/100\n",
      "34/34 - 0s - loss: 0.2100 - val_loss: 0.0836\n",
      "Epoch 84/100\n",
      "34/34 - 0s - loss: 0.1771 - val_loss: 0.0650\n",
      "Epoch 85/100\n",
      "34/34 - 0s - loss: 0.1868 - val_loss: 0.0663\n",
      "Epoch 86/100\n",
      "34/34 - 0s - loss: 0.1891 - val_loss: 0.0682\n",
      "Epoch 87/100\n",
      "34/34 - 0s - loss: 0.1650 - val_loss: 0.0678\n",
      "Epoch 88/100\n",
      "34/34 - 0s - loss: 0.1656 - val_loss: 0.0669\n",
      "Epoch 89/100\n",
      "34/34 - 0s - loss: 0.2061 - val_loss: 0.0664\n",
      "Epoch 90/100\n",
      "34/34 - 0s - loss: 0.1727 - val_loss: 0.0669\n",
      "Epoch 91/100\n",
      "34/34 - 0s - loss: 0.1815 - val_loss: 0.0683\n",
      "Epoch 92/100\n",
      "34/34 - 0s - loss: 0.1487 - val_loss: 0.0742\n",
      "Epoch 93/100\n",
      "34/34 - 0s - loss: 0.1709 - val_loss: 0.0916\n",
      "Epoch 94/100\n",
      "34/34 - 0s - loss: 0.1822 - val_loss: 0.1097\n",
      "Epoch 95/100\n",
      "34/34 - 0s - loss: 0.1860 - val_loss: 0.1220\n",
      "Epoch 96/100\n",
      "34/34 - 0s - loss: 0.1820 - val_loss: 0.1321\n",
      "Epoch 97/100\n",
      "34/34 - 0s - loss: 0.1604 - val_loss: 0.1377\n",
      "Epoch 98/100\n",
      "34/34 - 0s - loss: 0.1516 - val_loss: 0.1348\n",
      "Epoch 99/100\n",
      "34/34 - 0s - loss: 0.1912 - val_loss: 0.1333\n",
      "Epoch 100/100\n",
      "34/34 - 0s - loss: 0.1567 - val_loss: 0.1333\n",
      "Fold score (MAE): 0.3651440488243849\n",
      "Fold #5\n",
      "Train on 34 samples, validate on 8 samples\n",
      "Epoch 1/100\n",
      "34/34 - 0s - loss: 0.8211 - val_loss: 0.5079\n",
      "Epoch 2/100\n",
      "34/34 - 0s - loss: 0.5972 - val_loss: 0.3935\n",
      "Epoch 3/100\n",
      "34/34 - 0s - loss: 0.5245 - val_loss: 0.3199\n",
      "Epoch 4/100\n",
      "34/34 - 0s - loss: 0.3857 - val_loss: 0.2613\n",
      "Epoch 5/100\n",
      "34/34 - 0s - loss: 0.3468 - val_loss: 0.2670\n",
      "Epoch 6/100\n",
      "34/34 - 0s - loss: 0.3869 - val_loss: 0.3022\n",
      "Epoch 7/100\n",
      "34/34 - 0s - loss: 0.3773 - val_loss: 0.3230\n",
      "Epoch 8/100\n",
      "34/34 - 0s - loss: 0.3450 - val_loss: 0.3406\n",
      "Epoch 9/100\n",
      "34/34 - 0s - loss: 0.3504 - val_loss: 0.3427\n",
      "Epoch 10/100\n",
      "34/34 - 0s - loss: 0.2575 - val_loss: 0.3243\n",
      "Epoch 11/100\n",
      "34/34 - 0s - loss: 0.3249 - val_loss: 0.3079\n",
      "Epoch 12/100\n",
      "34/34 - 0s - loss: 0.2569 - val_loss: 0.3012\n",
      "Epoch 13/100\n",
      "34/34 - 0s - loss: 0.2667 - val_loss: 0.2920\n",
      "Epoch 14/100\n",
      "34/34 - 0s - loss: 0.2335 - val_loss: 0.3040\n",
      "Epoch 15/100\n",
      "34/34 - 0s - loss: 0.2574 - val_loss: 0.3243\n",
      "Epoch 16/100\n",
      "34/34 - 0s - loss: 0.2805 - val_loss: 0.3424\n",
      "Epoch 17/100\n",
      "34/34 - 0s - loss: 0.2361 - val_loss: 0.3577\n",
      "Epoch 18/100\n",
      "34/34 - 0s - loss: 0.3346 - val_loss: 0.3654\n",
      "Epoch 19/100\n",
      "34/34 - 0s - loss: 0.2964 - val_loss: 0.3634\n",
      "Epoch 20/100\n",
      "34/34 - 0s - loss: 0.2431 - val_loss: 0.3540\n",
      "Epoch 21/100\n",
      "34/34 - 0s - loss: 0.3112 - val_loss: 0.3399\n",
      "Epoch 22/100\n",
      "34/34 - 0s - loss: 0.2140 - val_loss: 0.3229\n",
      "Epoch 23/100\n",
      "34/34 - 0s - loss: 0.1743 - val_loss: 0.3055\n",
      "Epoch 24/100\n",
      "34/34 - 0s - loss: 0.1988 - val_loss: 0.3072\n",
      "Epoch 25/100\n",
      "34/34 - 0s - loss: 0.2553 - val_loss: 0.3287\n",
      "Epoch 26/100\n",
      "34/34 - 0s - loss: 0.2435 - val_loss: 0.3367\n",
      "Epoch 27/100\n",
      "34/34 - 0s - loss: 0.1898 - val_loss: 0.3367\n",
      "Epoch 28/100\n",
      "34/34 - 0s - loss: 0.1992 - val_loss: 0.3333\n",
      "Epoch 29/100\n",
      "34/34 - 0s - loss: 0.1811 - val_loss: 0.3257\n",
      "Epoch 30/100\n",
      "34/34 - 0s - loss: 0.1815 - val_loss: 0.3129\n",
      "Epoch 31/100\n",
      "34/34 - 0s - loss: 0.1816 - val_loss: 0.2993\n",
      "Epoch 32/100\n",
      "34/34 - 0s - loss: 0.1836 - val_loss: 0.3091\n",
      "Epoch 33/100\n",
      "34/34 - 0s - loss: 0.2040 - val_loss: 0.3159\n",
      "Epoch 34/100\n",
      "34/34 - 0s - loss: 0.1516 - val_loss: 0.3190\n",
      "Epoch 35/100\n",
      "34/34 - 0s - loss: 0.1451 - val_loss: 0.3187\n",
      "Epoch 36/100\n",
      "34/34 - 0s - loss: 0.1412 - val_loss: 0.3155\n",
      "Epoch 37/100\n",
      "34/34 - 0s - loss: 0.2076 - val_loss: 0.3047\n",
      "Epoch 38/100\n",
      "34/34 - 0s - loss: 0.1604 - val_loss: 0.2951\n",
      "Epoch 39/100\n",
      "34/34 - 0s - loss: 0.1641 - val_loss: 0.3092\n",
      "Epoch 40/100\n",
      "34/34 - 0s - loss: 0.1391 - val_loss: 0.3251\n",
      "Epoch 41/100\n",
      "34/34 - 0s - loss: 0.1817 - val_loss: 0.3373\n",
      "Epoch 42/100\n",
      "34/34 - 0s - loss: 0.1731 - val_loss: 0.3379\n",
      "Epoch 43/100\n",
      "34/34 - 0s - loss: 0.1749 - val_loss: 0.3317\n",
      "Epoch 44/100\n",
      "34/34 - 0s - loss: 0.1577 - val_loss: 0.3181\n",
      "Epoch 45/100\n",
      "34/34 - 0s - loss: 0.1540 - val_loss: 0.3133\n",
      "Epoch 46/100\n",
      "34/34 - 0s - loss: 0.1745 - val_loss: 0.3078\n",
      "Epoch 47/100\n",
      "34/34 - 0s - loss: 0.1442 - val_loss: 0.2941\n",
      "Epoch 48/100\n",
      "34/34 - 0s - loss: 0.1567 - val_loss: 0.2871\n",
      "Epoch 49/100\n",
      "34/34 - 0s - loss: 0.1182 - val_loss: 0.2973\n",
      "Epoch 50/100\n",
      "34/34 - 0s - loss: 0.1327 - val_loss: 0.3029\n",
      "Epoch 51/100\n",
      "34/34 - 0s - loss: 0.1355 - val_loss: 0.3055\n",
      "Epoch 52/100\n",
      "34/34 - 0s - loss: 0.1647 - val_loss: 0.3058\n",
      "Epoch 53/100\n",
      "34/34 - 0s - loss: 0.1601 - val_loss: 0.3017\n",
      "Epoch 54/100\n",
      "34/34 - 0s - loss: 0.1099 - val_loss: 0.2960\n",
      "Epoch 55/100\n",
      "34/34 - 0s - loss: 0.1538 - val_loss: 0.2933\n",
      "Epoch 56/100\n",
      "34/34 - 0s - loss: 0.1406 - val_loss: 0.2924\n",
      "Epoch 57/100\n",
      "34/34 - 0s - loss: 0.1242 - val_loss: 0.2916\n",
      "Epoch 58/100\n",
      "34/34 - 0s - loss: 0.1398 - val_loss: 0.2901\n",
      "Epoch 59/100\n",
      "34/34 - 0s - loss: 0.1602 - val_loss: 0.2893\n",
      "Epoch 60/100\n",
      "34/34 - 0s - loss: 0.1035 - val_loss: 0.2914\n",
      "Epoch 61/100\n",
      "34/34 - 0s - loss: 0.1162 - val_loss: 0.2928\n",
      "Epoch 62/100\n",
      "34/34 - 0s - loss: 0.1288 - val_loss: 0.2920\n",
      "Epoch 63/100\n",
      "34/34 - 0s - loss: 0.1494 - val_loss: 0.2877\n",
      "Epoch 64/100\n",
      "34/34 - 0s - loss: 0.1327 - val_loss: 0.2844\n",
      "Epoch 65/100\n",
      "34/34 - 0s - loss: 0.1298 - val_loss: 0.2824\n",
      "Epoch 66/100\n",
      "34/34 - 0s - loss: 0.1177 - val_loss: 0.2801\n",
      "Epoch 67/100\n",
      "34/34 - 0s - loss: 0.1021 - val_loss: 0.2792\n",
      "Epoch 68/100\n",
      "34/34 - 0s - loss: 0.1196 - val_loss: 0.2779\n",
      "Epoch 69/100\n",
      "34/34 - 0s - loss: 0.1005 - val_loss: 0.2769\n",
      "Epoch 70/100\n",
      "34/34 - 0s - loss: 0.1491 - val_loss: 0.2777\n",
      "Epoch 71/100\n",
      "34/34 - 0s - loss: 0.0900 - val_loss: 0.2798\n",
      "Epoch 72/100\n",
      "34/34 - 0s - loss: 0.1179 - val_loss: 0.2884\n",
      "Epoch 73/100\n",
      "34/34 - 0s - loss: 0.1220 - val_loss: 0.3006\n",
      "Epoch 74/100\n",
      "34/34 - 0s - loss: 0.1365 - val_loss: 0.3097\n",
      "Epoch 75/100\n",
      "34/34 - 0s - loss: 0.1395 - val_loss: 0.3090\n",
      "Epoch 76/100\n",
      "34/34 - 0s - loss: 0.1246 - val_loss: 0.3008\n",
      "Epoch 77/100\n",
      "34/34 - 0s - loss: 0.1349 - val_loss: 0.2916\n",
      "Epoch 78/100\n",
      "34/34 - 0s - loss: 0.1187 - val_loss: 0.2842\n",
      "Epoch 79/100\n",
      "34/34 - 0s - loss: 0.1179 - val_loss: 0.2843\n",
      "Epoch 80/100\n",
      "34/34 - 0s - loss: 0.1307 - val_loss: 0.2842\n",
      "Epoch 81/100\n",
      "34/34 - 0s - loss: 0.1207 - val_loss: 0.2839\n",
      "Epoch 82/100\n",
      "34/34 - 0s - loss: 0.1402 - val_loss: 0.2831\n",
      "Epoch 83/100\n",
      "34/34 - 0s - loss: 0.1304 - val_loss: 0.2825\n",
      "Epoch 84/100\n",
      "34/34 - 0s - loss: 0.1166 - val_loss: 0.2818\n",
      "Epoch 85/100\n",
      "34/34 - 0s - loss: 0.1285 - val_loss: 0.2813\n",
      "Epoch 86/100\n",
      "34/34 - 0s - loss: 0.1237 - val_loss: 0.2823\n",
      "Epoch 87/100\n",
      "34/34 - 0s - loss: 0.1190 - val_loss: 0.2840\n",
      "Epoch 88/100\n",
      "34/34 - 0s - loss: 0.1175 - val_loss: 0.2815\n",
      "Epoch 89/100\n",
      "34/34 - 0s - loss: 0.1183 - val_loss: 0.2803\n",
      "Epoch 90/100\n",
      "34/34 - 0s - loss: 0.1041 - val_loss: 0.2801\n",
      "Epoch 91/100\n",
      "34/34 - 0s - loss: 0.1590 - val_loss: 0.2896\n",
      "Epoch 92/100\n",
      "34/34 - 0s - loss: 0.1032 - val_loss: 0.3018\n",
      "Epoch 93/100\n",
      "34/34 - 0s - loss: 0.1106 - val_loss: 0.3084\n",
      "Epoch 94/100\n",
      "34/34 - 0s - loss: 0.1523 - val_loss: 0.3099\n",
      "Epoch 95/100\n",
      "34/34 - 0s - loss: 0.1135 - val_loss: 0.3022\n",
      "Epoch 96/100\n",
      "34/34 - 0s - loss: 0.1016 - val_loss: 0.2882\n",
      "Epoch 97/100\n",
      "34/34 - 0s - loss: 0.1324 - val_loss: 0.2699\n",
      "Epoch 98/100\n",
      "34/34 - 0s - loss: 0.1094 - val_loss: 0.2641\n",
      "Epoch 99/100\n",
      "34/34 - 0s - loss: 0.1056 - val_loss: 0.2684\n",
      "Epoch 100/100\n",
      "34/34 - 0s - loss: 0.1389 - val_loss: 0.2684\n",
      "Fold score (MAE): 0.51808964008025\n",
      "Final out of sample (oos) score (RMSE): 0.41261408508246983\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "kf = KFold(5, shuffle=True, random_state=42) # Use for KFold classification\n",
    "    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "for train, test in kf.split(x):\n",
    "    fold+=1\n",
    "    print(f\"Fold #{fold}\")\n",
    "        \n",
    "    x_train = x[train]  \n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=x.shape[1], activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(35, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    \n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),verbose=2,\n",
    "              epochs=100)\n",
    "    \n",
    "    pred = model.predict(x_test)\n",
    "    \n",
    "    oos_y.append(y_test)\n",
    "    oos_pred.append(pred)    \n",
    "\n",
    "    # Measure this fold's RMSE\n",
    "    score = np.sqrt(metrics.mean_absolute_error(pred,y_test))\n",
    "    print(f\"Fold score (MAE): {score}\")\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "\n",
    "score = np.sqrt(metrics.mean_absolute_error(oos_pred,oos_y))\n",
    "print(f\"Final out of sample (oos) score (RMSE): {score}\")    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.7/site-packages (3.1.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (46.0.0.post20200309)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib) (1.14.0)\n",
      "[0.859467 0.121883 0.003374 0.781081 0.049411 0.018807 0.006284 0.011044\n",
      " 0.08039  0.145164 0.015475]\n",
      "[[ 0.07531247]\n",
      " [ 0.6240422 ]\n",
      " [ 0.45373878]\n",
      " [ 0.6589144 ]\n",
      " [ 0.05861497]\n",
      " [ 0.09865056]\n",
      " [ 0.2869479 ]\n",
      " [ 0.16490218]\n",
      " [ 0.06701019]\n",
      " [ 0.10189034]\n",
      " [ 0.043685  ]\n",
      " [ 0.03821816]\n",
      " [ 0.06938758]\n",
      " [ 0.13491735]\n",
      " [ 0.07145759]\n",
      " [ 0.16163549]\n",
      " [ 0.12252794]\n",
      " [ 0.23923656]\n",
      " [ 0.9135694 ]\n",
      " [ 0.8973521 ]\n",
      " [ 0.81651866]\n",
      " [-0.01333419]\n",
      " [ 0.27541718]\n",
      " [ 0.03737716]\n",
      " [ 0.7657176 ]\n",
      " [ 0.00180522]\n",
      " [ 0.90138185]\n",
      " [ 0.7507064 ]\n",
      " [ 0.04380814]\n",
      " [ 0.06479006]\n",
      " [ 0.05796118]\n",
      " [ 0.00508351]\n",
      " [ 0.09632078]\n",
      " [ 0.00423269]\n",
      " [-0.02407946]\n",
      " [-0.01845404]\n",
      " [ 0.03762025]\n",
      " [ 0.6714478 ]\n",
      " [-0.01530963]\n",
      " [ 0.4217983 ]\n",
      " [ 0.00517353]\n",
      " [-0.01638996]]\n",
      "[1.31440e-02 8.96455e-01 7.81081e-01 1.00000e+00 6.28400e-03 3.89900e-02\n",
      " 4.37870e-01 1.08834e-01 9.71000e-04 1.21883e-01 2.15010e-02 0.00000e+00\n",
      " 4.94110e-02 1.44573e-01 6.60270e-02 1.88253e-01 1.45164e-01 2.86448e-01\n",
      " 9.31283e-01 9.18234e-01 8.59467e-01 1.17950e-02 2.92268e-01 8.03900e-02\n",
      " 7.59677e-01 1.45090e-02 9.90143e-01 8.59561e-01 1.92200e-02 1.88070e-02\n",
      " 6.83740e-02 9.82300e-03 4.96920e-02 1.54750e-02 3.37400e-03 8.64000e-03\n",
      " 8.17980e-02 7.72163e-01 1.31090e-02 4.66221e-01 1.10440e-02 3.86200e-03]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "df_1=pd.read_csv(\"/Users/philipsmith/Documents/validation.csv\",sep=\",\")\n",
    "\n",
    "df_1_2=df_1['VP'].values\n",
    "\n",
    "print(df_1_2)\n",
    "print(oos_pred)\n",
    "print(oos_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set R-Square= 0.47690472403759643\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(oos_pred, #training Y value\n",
    " oos_y, #predicted Y value\n",
    " 'ro') #red dots\n",
    "plt.title('Training Set')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "#Compute R-Square value for training set\n",
    "testR2=r2_score(oos_pred, oos_y)\n",
    "print(\"Training Set R-Square=\", testR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.931283 0.918234 0.990143 0.859561 0.013144 0.859467 0.121883 0.003374\n",
      " 0.896455 0.021501 0.00864 ]\n",
      "[0.859467 0.121883 0.003374 0.781081 0.049411 0.018807 0.006284 0.011044\n",
      " 0.08039  0.145164 0.015475]\n"
     ]
    }
   ],
   "source": [
    "oos_pred_2=y[0:11]\n",
    "oos_pred_3=np.asarray(oos_pred_2)\n",
    "print(oos_pred_3)\n",
    "print(df_1_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set R-Square= -0.43051958335039475\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(oos_pred_2, #training Y value\n",
    " df_1_2, #predicted Y value\n",
    " 'ro') #red dots\n",
    "plt.title('Validation Set')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "#Compute R-Square value for training set\n",
    "testR2=r2_score(oos_pred_2,df_1_2)\n",
    "print(\"Validation Set R-Square=\", testR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
