{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): failed\n",
      "\n",
      "NoSpaceLeftError: No space left on devices.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge keras --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - conda\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    absl-py-0.13.0             |   py37hecd8cb5_0         176 KB\n",
      "    astor-0.8.1                |   py37hecd8cb5_0          47 KB\n",
      "    backports.functools_lru_cache-1.6.4|     pyhd3eb1b0_0           9 KB\n",
      "    backports.tempfile-1.0     |     pyhd3eb1b0_1          11 KB\n",
      "    c-ares-1.17.1              |       h9ed2024_0          92 KB\n",
      "    conda-4.10.3               |   py37hecd8cb5_0         2.9 MB\n",
      "    conda-package-handling-1.7.3|   py37h9ed2024_1         1.5 MB\n",
      "    gast-0.5.0                 |     pyhd3eb1b0_0          13 KB\n",
      "    google-pasta-0.2.0         |             py_0          46 KB\n",
      "    importlib-metadata-3.10.0  |   py37hecd8cb5_0          33 KB\n",
      "    keras-applications-1.0.8   |             py_1          29 KB\n",
      "    keras-preprocessing-1.1.2  |     pyhd3eb1b0_0          35 KB\n",
      "    llvm-4.0.1                 |                1        43.0 MB\n",
      "    markdown-3.3.4             |   py37hecd8cb5_0         127 KB\n",
      "    termcolor-1.1.0            |   py37hecd8cb5_1           9 KB\n",
      "    theano-1.0.4               |   py37h04f5b5a_0         3.1 MB\n",
      "    typing_extensions-3.10.0.0 |     pyh06a4308_0          27 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        51.1 MB\n",
      "\n",
      "The following packages will be REMOVED:\n",
      "\n",
      "  libgpuarray-0.7.6-h0d85af4_1003\n",
      "  mako-1.1.4-pyh44b312d_0\n",
      "  pygpu-0.7.6-py37h9f9e6dc_1002\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  backports.functoo~                             1.6.1-py_0 --> 1.6.4-pyhd3eb1b0_0\n",
      "  conda-package-han~                   1.6.1-py37h1de35cc_0 --> 1.7.3-py37h9ed2024_1\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  absl-py            conda-forge/noarch::absl-py-0.13.0-py~ --> pkgs/main/osx-64::absl-py-0.13.0-py37hecd8cb5_0\n",
      "  astor              conda-forge/noarch::astor-0.8.1-pyh9f~ --> pkgs/main/osx-64::astor-0.8.1-py37hecd8cb5_0\n",
      "  c-ares              conda-forge::c-ares-1.17.1-h0d85af4_1 --> pkgs/main::c-ares-1.17.1-h9ed2024_0\n",
      "  conda              conda-forge::conda-4.10.3-py37hf98548~ --> pkgs/main::conda-4.10.3-py37hecd8cb5_0\n",
      "  gast                 conda-forge::gast-0.5.0-pyhd8ed1ab_0 --> pkgs/main::gast-0.5.0-pyhd3eb1b0_0\n",
      "  google-pasta       conda-forge::google-pasta-0.2.0-pyh8c~ --> pkgs/main::google-pasta-0.2.0-py_0\n",
      "  importlib-metadata conda-forge::importlib-metadata-4.6.1~ --> pkgs/main::importlib-metadata-3.10.0-py37hecd8cb5_0\n",
      "  keras-applications                            conda-forge --> pkgs/main\n",
      "  keras-preprocessi~ conda-forge::keras-preprocessing-1.1.~ --> pkgs/main::keras-preprocessing-1.1.2-pyhd3eb1b0_0\n",
      "  llvm                                          conda-forge --> pkgs/main\n",
      "  markdown           conda-forge/noarch::markdown-3.3.4-py~ --> pkgs/main/osx-64::markdown-3.3.4-py37hecd8cb5_0\n",
      "  termcolor          conda-forge/noarch::termcolor-1.1.0-p~ --> pkgs/main/osx-64::termcolor-1.1.0-py37hecd8cb5_1\n",
      "  theano             conda-forge::theano-1.0.4-py37h0a4402~ --> pkgs/main::theano-1.0.4-py37h04f5b5a_0\n",
      "  typing_extensions  conda-forge::typing_extensions-3.10.0~ --> pkgs/main::typing_extensions-3.10.0.0-pyh06a4308_0\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "  backports.tempfile                               1.0-py_1 --> 1.0-pyhd3eb1b0_1\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? ^C\n",
      "\n",
      "CondaSystemExit: \n",
      "Operation aborted.  Exiting.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda update -n base -c defaults conda\n",
    "!conda install -c conda-forge tensorflow --yes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.datasets import imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data( num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import r2_score \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.7/site-packages (1.0.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/anaconda3/lib/python3.7/site-packages (from pandas) (2019.3)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/anaconda3/lib/python3.7/site-packages (from pandas) (1.18.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/anaconda3/lib/python3.7/site-packages (from pandas) (2.8.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\r\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Xt</th>\n",
       "      <th>Psi_i_t</th>\n",
       "      <th>SpMaxA_X</th>\n",
       "      <th>Chi_Dt</th>\n",
       "      <th>VE2_X</th>\n",
       "      <th>VE2sign_L</th>\n",
       "      <th>SpMaxA_AEA(ri)</th>\n",
       "      <th>VP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.561</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.931283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.601</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.918234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.693</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.990143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.588</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.859561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.363</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.013144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.849</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.859467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.406</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.121883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.348</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.003374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.675</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.896455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.406</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.021501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.400</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.008640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.011795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.329</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.393</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.781081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.431</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.081798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.342</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.049411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.410</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.144573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.374</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.066027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.449</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.772163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.693</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.645</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.317</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.013109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.374</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.019220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.406</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.466221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.406</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.018807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.397</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.292268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.288</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.006284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.366</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.038990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.366</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.188253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.484</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.011044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.230</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.437870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.108834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.329</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.080390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.319</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.068374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.385</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.145164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.309</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.009823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.508</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.759677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.385</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.286448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.431</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.049692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.420</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.003862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.385</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.000971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.357</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.014509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.331</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.015475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.849</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.859467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.406</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.121883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.348</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.003374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.393</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.781081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.342</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.049411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.406</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.018807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.288</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.006284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.484</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.011044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.329</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.080390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.385</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.145164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.331</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.015475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Xt  Psi_i_t  SpMaxA_X  Chi_Dt  VE2_X  VE2sign_L  SpMaxA_AEA(ri)  \\\n",
       "0   0.561    0.054     0.143   0.472  0.370      0.378           0.547   \n",
       "1   0.601    0.071     0.167   0.468  0.404      0.408           0.540   \n",
       "2   0.693    0.115     0.200   0.548  0.441      0.447           0.636   \n",
       "3   0.588    0.050     0.143   0.492  0.366      0.378           0.561   \n",
       "4   0.363    0.001     0.077   0.225  0.270      0.277           0.314   \n",
       "5   0.849    0.128     0.250   0.658  0.493      0.500           0.781   \n",
       "6   0.406    0.008     0.100   0.249  0.310      0.316           0.389   \n",
       "7   0.348    0.002     0.071   0.220  0.261      0.267           0.293   \n",
       "8   0.675    0.035     0.167   0.599  0.393      0.408           0.698   \n",
       "9   0.406    0.007     0.100   0.249  0.310      0.316           0.391   \n",
       "10  0.400    0.019     0.091   0.269  0.299      0.302           0.278   \n",
       "11  0.350    0.005     0.077   0.210  0.273      0.277           0.293   \n",
       "12  0.329    0.002     0.067   0.206  0.254      0.258           0.257   \n",
       "13  0.393    0.004     0.091   0.247  0.294      0.302           0.348   \n",
       "14  0.431    0.013     0.111   0.260  0.326      0.333           0.416   \n",
       "15  0.342    0.003     0.062   0.219  0.240      0.250           0.278   \n",
       "16  0.410    0.013     0.083   0.301  0.281      0.289           0.369   \n",
       "17  0.374    0.005     0.083   0.230  0.282      0.289           0.333   \n",
       "18  0.449    0.036     0.125   0.268  0.348      0.354           0.442   \n",
       "19  0.693    0.068     0.200   0.548  0.441      0.447           0.645   \n",
       "20  0.317    0.000     0.059   0.232  0.236      0.243           0.268   \n",
       "21  0.374    0.002     0.083   0.235  0.282      0.289           0.322   \n",
       "22  0.406    0.006     0.100   0.253  0.310      0.316           0.377   \n",
       "23  0.406    0.009     0.100   0.258  0.310      0.316           0.377   \n",
       "24  0.397    0.015     0.100   0.251  0.312      0.316           0.362   \n",
       "25  0.288    0.001     0.059   0.172  0.240      0.243           0.219   \n",
       "26  0.366    0.006     0.083   0.227  0.284      0.289           0.319   \n",
       "27  0.366    0.008     0.083   0.228  0.284      0.289           0.319   \n",
       "28  0.484    0.008     0.111   0.364  0.325      0.333           0.413   \n",
       "29  0.230    0.000     0.037   0.152  0.190      0.192           0.149   \n",
       "30  0.350    0.010     0.091   0.174  0.271      0.302           0.324   \n",
       "31  0.329    0.003     0.067   0.205  0.254      0.258           0.262   \n",
       "32  0.319    0.015     0.071   0.195  0.263      0.267           0.297   \n",
       "33  0.385    0.008     0.091   0.242  0.296      0.302           0.358   \n",
       "34  0.309    0.000     0.053   0.201  0.222      0.229           0.234   \n",
       "35  0.508    0.036     0.125   0.417  0.347      0.354           0.468   \n",
       "36  0.385    0.012     0.091   0.248  0.296      0.302           0.345   \n",
       "37  0.431    0.011     0.111   0.259  0.326      0.333           0.410   \n",
       "38  0.420    0.005     0.091   0.291  0.295      0.302           0.327   \n",
       "39  0.385    0.012     0.083   0.299  0.280      0.289           0.385   \n",
       "40  0.357    0.001     0.067   0.274  0.252      0.258           0.252   \n",
       "41  0.331    0.000     0.062   0.196  0.247      0.250           0.214   \n",
       "42  0.849    0.128     0.250   0.658  0.493      0.500           0.781   \n",
       "43  0.406    0.008     0.100   0.249  0.310      0.316           0.389   \n",
       "44  0.348    0.002     0.071   0.220  0.261      0.267           0.293   \n",
       "45  0.393    0.004     0.091   0.247  0.294      0.302           0.348   \n",
       "46  0.342    0.003     0.062   0.219  0.240      0.250           0.278   \n",
       "47  0.406    0.009     0.100   0.258  0.310      0.316           0.377   \n",
       "48  0.288    0.001     0.059   0.172  0.240      0.243           0.219   \n",
       "49  0.484    0.008     0.111   0.364  0.325      0.333           0.413   \n",
       "50  0.329    0.003     0.067   0.205  0.254      0.258           0.262   \n",
       "51  0.385    0.008     0.091   0.242  0.296      0.302           0.358   \n",
       "52  0.331    0.000     0.062   0.196  0.247      0.250           0.214   \n",
       "\n",
       "          VP  \n",
       "0   0.931283  \n",
       "1   0.918234  \n",
       "2   0.990143  \n",
       "3   0.859561  \n",
       "4   0.013144  \n",
       "5   0.859467  \n",
       "6   0.121883  \n",
       "7   0.003374  \n",
       "8   0.896455  \n",
       "9   0.021501  \n",
       "10  0.008640  \n",
       "11  0.011795  \n",
       "12  0.000000  \n",
       "13  0.781081  \n",
       "14  0.081798  \n",
       "15  0.049411  \n",
       "16  0.144573  \n",
       "17  0.066027  \n",
       "18  0.772163  \n",
       "19  1.000000  \n",
       "20  0.013109  \n",
       "21  0.019220  \n",
       "22  0.466221  \n",
       "23  0.018807  \n",
       "24  0.292268  \n",
       "25  0.006284  \n",
       "26  0.038990  \n",
       "27  0.188253  \n",
       "28  0.011044  \n",
       "29  0.437870  \n",
       "30  0.108834  \n",
       "31  0.080390  \n",
       "32  0.068374  \n",
       "33  0.145164  \n",
       "34  0.009823  \n",
       "35  0.759677  \n",
       "36  0.286448  \n",
       "37  0.049692  \n",
       "38  0.003862  \n",
       "39  0.000971  \n",
       "40  0.014509  \n",
       "41  0.015475  \n",
       "42  0.859467  \n",
       "43  0.121883  \n",
       "44  0.003374  \n",
       "45  0.781081  \n",
       "46  0.049411  \n",
       "47  0.018807  \n",
       "48  0.006284  \n",
       "49  0.011044  \n",
       "50  0.080390  \n",
       "51  0.145164  \n",
       "52  0.015475  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"/Users/philipsmith/Documents/training_4.csv\",sep=\",\")\n",
    "df\n",
    "#header = next(training_file)\n",
    "#delimiter = \",\"\n",
    "#skip_header = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_columns=df.columns\n",
    "x = df[x_columns].values\n",
    "y = df['VP'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "Train on 42 samples, validate on 11 samples\n",
      "Epoch 1/35\n",
      "42/42 - 0s - loss: 0.2414 - val_loss: 0.3107\n",
      "Epoch 2/35\n",
      "42/42 - 0s - loss: 0.2251 - val_loss: 0.2946\n",
      "Epoch 3/35\n",
      "42/42 - 0s - loss: 0.2134 - val_loss: 0.2816\n",
      "Epoch 4/35\n",
      "42/42 - 0s - loss: 0.2075 - val_loss: 0.2715\n",
      "Epoch 5/35\n",
      "42/42 - 0s - loss: 0.2018 - val_loss: 0.2626\n",
      "Epoch 6/35\n",
      "42/42 - 0s - loss: 0.1964 - val_loss: 0.2529\n",
      "Epoch 7/35\n",
      "42/42 - 0s - loss: 0.1765 - val_loss: 0.2433\n",
      "Epoch 8/35\n",
      "42/42 - 0s - loss: 0.1886 - val_loss: 0.2328\n",
      "Epoch 9/35\n",
      "42/42 - 0s - loss: 0.1839 - val_loss: 0.2208\n",
      "Epoch 10/35\n",
      "42/42 - 0s - loss: 0.1766 - val_loss: 0.2087\n",
      "Epoch 11/35\n",
      "42/42 - 0s - loss: 0.1705 - val_loss: 0.1991\n",
      "Epoch 12/35\n",
      "42/42 - 0s - loss: 0.1349 - val_loss: 0.1917\n",
      "Epoch 13/35\n",
      "42/42 - 0s - loss: 0.1485 - val_loss: 0.1853\n",
      "Epoch 14/35\n",
      "42/42 - 0s - loss: 0.1466 - val_loss: 0.1734\n",
      "Epoch 15/35\n",
      "42/42 - 0s - loss: 0.1278 - val_loss: 0.1635\n",
      "Epoch 16/35\n",
      "42/42 - 0s - loss: 0.1145 - val_loss: 0.1603\n",
      "Epoch 17/35\n",
      "42/42 - 0s - loss: 0.1239 - val_loss: 0.1575\n",
      "Epoch 18/35\n",
      "42/42 - 0s - loss: 0.1139 - val_loss: 0.1520\n",
      "Epoch 19/35\n",
      "42/42 - 0s - loss: 0.1115 - val_loss: 0.1410\n",
      "Epoch 20/35\n",
      "42/42 - 0s - loss: 0.1010 - val_loss: 0.1284\n",
      "Epoch 21/35\n",
      "42/42 - 0s - loss: 0.1125 - val_loss: 0.1156\n",
      "Epoch 22/35\n",
      "42/42 - 0s - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 23/35\n",
      "42/42 - 0s - loss: 0.0818 - val_loss: 0.0927\n",
      "Epoch 24/35\n",
      "42/42 - 0s - loss: 0.0839 - val_loss: 0.0940\n",
      "Epoch 25/35\n",
      "42/42 - 0s - loss: 0.0798 - val_loss: 0.0762\n",
      "Epoch 26/35\n",
      "42/42 - 0s - loss: 0.0878 - val_loss: 0.0631\n",
      "Epoch 27/35\n",
      "42/42 - 0s - loss: 0.0832 - val_loss: 0.0614\n",
      "Epoch 28/35\n",
      "42/42 - 0s - loss: 0.0703 - val_loss: 0.0543\n",
      "Epoch 29/35\n",
      "42/42 - 0s - loss: 0.0808 - val_loss: 0.0437\n",
      "Epoch 30/35\n",
      "42/42 - 0s - loss: 0.0640 - val_loss: 0.0326\n",
      "Epoch 31/35\n",
      "42/42 - 0s - loss: 0.0571 - val_loss: 0.0249\n",
      "Epoch 32/35\n",
      "42/42 - 0s - loss: 0.0685 - val_loss: 0.0236\n",
      "Epoch 33/35\n",
      "42/42 - 0s - loss: 0.0632 - val_loss: 0.0229\n",
      "Epoch 34/35\n",
      "42/42 - 0s - loss: 0.0523 - val_loss: 0.0232\n",
      "Epoch 35/35\n",
      "42/42 - 0s - loss: 0.0566 - val_loss: 0.0249\n",
      "Fold score (MAE): 0.15775940687904993\n",
      "Fold #2\n",
      "Train on 42 samples, validate on 11 samples\n",
      "Epoch 1/35\n",
      "42/42 - 0s - loss: 0.2444 - val_loss: 0.1111\n",
      "Epoch 2/35\n",
      "42/42 - 0s - loss: 0.2334 - val_loss: 0.1141\n",
      "Epoch 3/35\n",
      "42/42 - 0s - loss: 0.2468 - val_loss: 0.1147\n",
      "Epoch 4/35\n",
      "42/42 - 0s - loss: 0.2162 - val_loss: 0.1126\n",
      "Epoch 5/35\n",
      "42/42 - 0s - loss: 0.2290 - val_loss: 0.1058\n",
      "Epoch 6/35\n",
      "42/42 - 0s - loss: 0.2207 - val_loss: 0.0967\n",
      "Epoch 7/35\n",
      "42/42 - 0s - loss: 0.1632 - val_loss: 0.0897\n",
      "Epoch 8/35\n",
      "42/42 - 0s - loss: 0.1742 - val_loss: 0.0840\n",
      "Epoch 9/35\n",
      "42/42 - 0s - loss: 0.1736 - val_loss: 0.0779\n",
      "Epoch 10/35\n",
      "42/42 - 0s - loss: 0.1526 - val_loss: 0.0694\n",
      "Epoch 11/35\n",
      "42/42 - 0s - loss: 0.1230 - val_loss: 0.0618\n",
      "Epoch 12/35\n",
      "42/42 - 0s - loss: 0.1203 - val_loss: 0.0548\n",
      "Epoch 13/35\n",
      "42/42 - 0s - loss: 0.1082 - val_loss: 0.0484\n",
      "Epoch 14/35\n",
      "42/42 - 0s - loss: 0.0990 - val_loss: 0.0373\n",
      "Epoch 15/35\n",
      "42/42 - 0s - loss: 0.1040 - val_loss: 0.0225\n",
      "Epoch 16/35\n",
      "42/42 - 0s - loss: 0.0784 - val_loss: 0.0288\n",
      "Epoch 17/35\n",
      "42/42 - 0s - loss: 0.1029 - val_loss: 0.0483\n",
      "Epoch 18/35\n",
      "42/42 - 0s - loss: 0.0942 - val_loss: 0.0537\n",
      "Epoch 19/35\n",
      "42/42 - 0s - loss: 0.1168 - val_loss: 0.0450\n",
      "Epoch 20/35\n",
      "42/42 - 0s - loss: 0.0880 - val_loss: 0.0269\n",
      "Epoch 21/35\n",
      "42/42 - 0s - loss: 0.0912 - val_loss: 0.0153\n",
      "Epoch 22/35\n",
      "42/42 - 0s - loss: 0.0850 - val_loss: 0.0064\n",
      "Epoch 23/35\n",
      "42/42 - 0s - loss: 0.0906 - val_loss: 0.0162\n",
      "Epoch 24/35\n",
      "42/42 - 0s - loss: 0.0804 - val_loss: 0.0109\n",
      "Epoch 25/35\n",
      "42/42 - 0s - loss: 0.0788 - val_loss: 0.0119\n",
      "Epoch 26/35\n",
      "42/42 - 0s - loss: 0.0785 - val_loss: 0.0302\n",
      "Epoch 27/35\n",
      "42/42 - 0s - loss: 0.0827 - val_loss: 0.0433\n",
      "Epoch 28/35\n",
      "42/42 - 0s - loss: 0.0601 - val_loss: 0.0493\n",
      "Epoch 29/35\n",
      "42/42 - 0s - loss: 0.0844 - val_loss: 0.0496\n",
      "Epoch 30/35\n",
      "42/42 - 0s - loss: 0.0879 - val_loss: 0.0404\n",
      "Epoch 31/35\n",
      "42/42 - 0s - loss: 0.0807 - val_loss: 0.0262\n",
      "Epoch 32/35\n",
      "42/42 - 0s - loss: 0.0710 - val_loss: 0.0162\n",
      "Epoch 33/35\n",
      "42/42 - 0s - loss: 0.0593 - val_loss: 0.0159\n",
      "Epoch 34/35\n",
      "42/42 - 0s - loss: 0.0639 - val_loss: 0.0173\n",
      "Epoch 35/35\n",
      "42/42 - 0s - loss: 0.0770 - val_loss: 0.0177\n",
      "Fold score (MAE): 0.13295815523929236\n",
      "Fold #3\n",
      "Train on 42 samples, validate on 11 samples\n",
      "Epoch 1/35\n",
      "42/42 - 0s - loss: 0.2329 - val_loss: 0.1205\n",
      "Epoch 2/35\n",
      "42/42 - 0s - loss: 0.2195 - val_loss: 0.1149\n",
      "Epoch 3/35\n",
      "42/42 - 0s - loss: 0.2052 - val_loss: 0.1098\n",
      "Epoch 4/35\n",
      "42/42 - 0s - loss: 0.1988 - val_loss: 0.1015\n",
      "Epoch 5/35\n",
      "42/42 - 0s - loss: 0.1823 - val_loss: 0.0934\n",
      "Epoch 6/35\n",
      "42/42 - 0s - loss: 0.1914 - val_loss: 0.0866\n",
      "Epoch 7/35\n",
      "42/42 - 0s - loss: 0.1749 - val_loss: 0.0816\n",
      "Epoch 8/35\n",
      "42/42 - 0s - loss: 0.1658 - val_loss: 0.0774\n",
      "Epoch 9/35\n",
      "42/42 - 0s - loss: 0.1398 - val_loss: 0.0734\n",
      "Epoch 10/35\n",
      "42/42 - 0s - loss: 0.1475 - val_loss: 0.0682\n",
      "Epoch 11/35\n",
      "42/42 - 0s - loss: 0.1295 - val_loss: 0.0618\n",
      "Epoch 12/35\n",
      "42/42 - 0s - loss: 0.1343 - val_loss: 0.0549\n",
      "Epoch 13/35\n",
      "42/42 - 0s - loss: 0.1153 - val_loss: 0.0484\n",
      "Epoch 14/35\n",
      "42/42 - 0s - loss: 0.0844 - val_loss: 0.0421\n",
      "Epoch 15/35\n",
      "42/42 - 0s - loss: 0.0904 - val_loss: 0.0344\n",
      "Epoch 16/35\n",
      "42/42 - 0s - loss: 0.0923 - val_loss: 0.0324\n",
      "Epoch 17/35\n",
      "42/42 - 0s - loss: 0.0837 - val_loss: 0.0269\n",
      "Epoch 18/35\n",
      "42/42 - 0s - loss: 0.0847 - val_loss: 0.0210\n",
      "Epoch 19/35\n",
      "42/42 - 0s - loss: 0.0690 - val_loss: 0.0250\n",
      "Epoch 20/35\n",
      "42/42 - 0s - loss: 0.0732 - val_loss: 0.0339\n",
      "Epoch 21/35\n",
      "42/42 - 0s - loss: 0.0654 - val_loss: 0.0406\n",
      "Epoch 22/35\n",
      "42/42 - 0s - loss: 0.0590 - val_loss: 0.0418\n",
      "Epoch 23/35\n",
      "42/42 - 0s - loss: 0.0748 - val_loss: 0.0314\n",
      "Epoch 24/35\n",
      "42/42 - 0s - loss: 0.0524 - val_loss: 0.0148\n",
      "Epoch 25/35\n",
      "42/42 - 0s - loss: 0.0668 - val_loss: 0.0088\n",
      "Epoch 26/35\n",
      "42/42 - 0s - loss: 0.0853 - val_loss: 0.0094\n",
      "Epoch 27/35\n",
      "42/42 - 0s - loss: 0.0665 - val_loss: 0.0079\n",
      "Epoch 28/35\n",
      "42/42 - 0s - loss: 0.0468 - val_loss: 0.0149\n",
      "Epoch 29/35\n",
      "42/42 - 0s - loss: 0.0556 - val_loss: 0.0277\n",
      "Epoch 30/35\n",
      "42/42 - 0s - loss: 0.0554 - val_loss: 0.0340\n",
      "Epoch 31/35\n",
      "42/42 - 0s - loss: 0.0480 - val_loss: 0.0359\n",
      "Epoch 32/35\n",
      "42/42 - 0s - loss: 0.0625 - val_loss: 0.0323\n",
      "Epoch 33/35\n",
      "42/42 - 0s - loss: 0.0632 - val_loss: 0.0218\n",
      "Epoch 34/35\n",
      "42/42 - 0s - loss: 0.0490 - val_loss: 0.0115\n",
      "Epoch 35/35\n",
      "42/42 - 0s - loss: 0.0618 - val_loss: 0.0129\n",
      "Fold score (MAE): 0.11362459346260682\n",
      "Fold #4\n",
      "Train on 43 samples, validate on 10 samples\n",
      "Epoch 1/35\n",
      "43/43 - 0s - loss: 0.2639 - val_loss: 0.4137\n",
      "Epoch 2/35\n",
      "43/43 - 0s - loss: 0.2283 - val_loss: 0.3740\n",
      "Epoch 3/35\n",
      "43/43 - 0s - loss: 0.2036 - val_loss: 0.3605\n",
      "Epoch 4/35\n",
      "43/43 - 0s - loss: 0.2265 - val_loss: 0.3491\n",
      "Epoch 5/35\n",
      "43/43 - 0s - loss: 0.1943 - val_loss: 0.3372\n",
      "Epoch 6/35\n",
      "43/43 - 0s - loss: 0.2081 - val_loss: 0.3247\n",
      "Epoch 7/35\n",
      "43/43 - 0s - loss: 0.1918 - val_loss: 0.3123\n",
      "Epoch 8/35\n",
      "43/43 - 0s - loss: 0.1780 - val_loss: 0.3001\n",
      "Epoch 9/35\n",
      "43/43 - 0s - loss: 0.2025 - val_loss: 0.2884\n",
      "Epoch 10/35\n",
      "43/43 - 0s - loss: 0.1909 - val_loss: 0.2775\n",
      "Epoch 11/35\n",
      "43/43 - 0s - loss: 0.1575 - val_loss: 0.2676\n",
      "Epoch 12/35\n",
      "43/43 - 0s - loss: 0.1812 - val_loss: 0.2582\n",
      "Epoch 13/35\n",
      "43/43 - 0s - loss: 0.1580 - val_loss: 0.2484\n",
      "Epoch 14/35\n",
      "43/43 - 0s - loss: 0.1524 - val_loss: 0.2375\n",
      "Epoch 15/35\n",
      "43/43 - 0s - loss: 0.1436 - val_loss: 0.2254\n",
      "Epoch 16/35\n",
      "43/43 - 0s - loss: 0.1358 - val_loss: 0.2133\n",
      "Epoch 17/35\n",
      "43/43 - 0s - loss: 0.1444 - val_loss: 0.2006\n",
      "Epoch 18/35\n",
      "43/43 - 0s - loss: 0.1348 - val_loss: 0.1880\n",
      "Epoch 19/35\n",
      "43/43 - 0s - loss: 0.1311 - val_loss: 0.1755\n",
      "Epoch 20/35\n",
      "43/43 - 0s - loss: 0.1378 - val_loss: 0.1643\n",
      "Epoch 21/35\n",
      "43/43 - 0s - loss: 0.1122 - val_loss: 0.1534\n",
      "Epoch 22/35\n",
      "43/43 - 0s - loss: 0.1000 - val_loss: 0.1421\n",
      "Epoch 23/35\n",
      "43/43 - 0s - loss: 0.0925 - val_loss: 0.1306\n",
      "Epoch 24/35\n",
      "43/43 - 0s - loss: 0.1012 - val_loss: 0.1175\n",
      "Epoch 25/35\n",
      "43/43 - 0s - loss: 0.1000 - val_loss: 0.1049\n",
      "Epoch 26/35\n",
      "43/43 - 0s - loss: 0.0928 - val_loss: 0.0945\n",
      "Epoch 27/35\n",
      "43/43 - 0s - loss: 0.0861 - val_loss: 0.0871\n",
      "Epoch 28/35\n",
      "43/43 - 0s - loss: 0.0797 - val_loss: 0.0700\n",
      "Epoch 29/35\n",
      "43/43 - 0s - loss: 0.0780 - val_loss: 0.0589\n",
      "Epoch 30/35\n",
      "43/43 - 0s - loss: 0.0710 - val_loss: 0.0563\n",
      "Epoch 31/35\n",
      "43/43 - 0s - loss: 0.0644 - val_loss: 0.0533\n",
      "Epoch 32/35\n",
      "43/43 - 0s - loss: 0.0635 - val_loss: 0.0458\n",
      "Epoch 33/35\n",
      "43/43 - 0s - loss: 0.0777 - val_loss: 0.0437\n",
      "Epoch 34/35\n",
      "43/43 - 0s - loss: 0.0802 - val_loss: 0.0432\n",
      "Epoch 35/35\n",
      "43/43 - 0s - loss: 0.0842 - val_loss: 0.0448\n",
      "Fold score (MAE): 0.21166586786510522\n",
      "Fold #5\n",
      "Train on 43 samples, validate on 10 samples\n",
      "Epoch 1/35\n",
      "43/43 - 0s - loss: 0.2622 - val_loss: 0.2042\n",
      "Epoch 2/35\n",
      "43/43 - 0s - loss: 0.2346 - val_loss: 0.1803\n",
      "Epoch 3/35\n",
      "43/43 - 0s - loss: 0.2225 - val_loss: 0.1725\n",
      "Epoch 4/35\n",
      "43/43 - 0s - loss: 0.1934 - val_loss: 0.1698\n",
      "Epoch 5/35\n",
      "43/43 - 0s - loss: 0.1917 - val_loss: 0.1655\n",
      "Epoch 6/35\n",
      "43/43 - 0s - loss: 0.1724 - val_loss: 0.1603\n",
      "Epoch 7/35\n",
      "43/43 - 0s - loss: 0.2006 - val_loss: 0.1534\n",
      "Epoch 8/35\n",
      "43/43 - 0s - loss: 0.1678 - val_loss: 0.1456\n",
      "Epoch 9/35\n",
      "43/43 - 0s - loss: 0.1550 - val_loss: 0.1373\n",
      "Epoch 10/35\n",
      "43/43 - 0s - loss: 0.1780 - val_loss: 0.1290\n",
      "Epoch 11/35\n",
      "43/43 - 0s - loss: 0.1535 - val_loss: 0.1211\n",
      "Epoch 12/35\n",
      "43/43 - 0s - loss: 0.1477 - val_loss: 0.1130\n",
      "Epoch 13/35\n",
      "43/43 - 0s - loss: 0.1197 - val_loss: 0.1047\n",
      "Epoch 14/35\n",
      "43/43 - 0s - loss: 0.1334 - val_loss: 0.0976\n",
      "Epoch 15/35\n",
      "43/43 - 0s - loss: 0.1389 - val_loss: 0.0903\n",
      "Epoch 16/35\n",
      "43/43 - 0s - loss: 0.1079 - val_loss: 0.0827\n",
      "Epoch 17/35\n",
      "43/43 - 0s - loss: 0.1123 - val_loss: 0.0751\n",
      "Epoch 18/35\n",
      "43/43 - 0s - loss: 0.1083 - val_loss: 0.0675\n",
      "Epoch 19/35\n",
      "43/43 - 0s - loss: 0.0897 - val_loss: 0.0613\n",
      "Epoch 20/35\n",
      "43/43 - 0s - loss: 0.1204 - val_loss: 0.0544\n",
      "Epoch 21/35\n",
      "43/43 - 0s - loss: 0.0901 - val_loss: 0.0475\n",
      "Epoch 22/35\n",
      "43/43 - 0s - loss: 0.0749 - val_loss: 0.0401\n",
      "Epoch 23/35\n",
      "43/43 - 0s - loss: 0.1043 - val_loss: 0.0356\n",
      "Epoch 24/35\n",
      "43/43 - 0s - loss: 0.0509 - val_loss: 0.0301\n",
      "Epoch 25/35\n",
      "43/43 - 0s - loss: 0.0797 - val_loss: 0.0256\n",
      "Epoch 26/35\n",
      "43/43 - 0s - loss: 0.0707 - val_loss: 0.0295\n",
      "Epoch 27/35\n",
      "43/43 - 0s - loss: 0.0789 - val_loss: 0.0345\n",
      "Epoch 28/35\n",
      "43/43 - 0s - loss: 0.0824 - val_loss: 0.0367\n",
      "Epoch 29/35\n",
      "43/43 - 0s - loss: 0.0775 - val_loss: 0.0377\n",
      "Epoch 30/35\n",
      "43/43 - 0s - loss: 0.0762 - val_loss: 0.0321\n",
      "Epoch 31/35\n",
      "43/43 - 0s - loss: 0.0699 - val_loss: 0.0239\n",
      "Epoch 32/35\n",
      "43/43 - 0s - loss: 0.0894 - val_loss: 0.0218\n",
      "Epoch 33/35\n",
      "43/43 - 0s - loss: 0.0602 - val_loss: 0.0228\n",
      "Epoch 34/35\n",
      "43/43 - 0s - loss: 0.0813 - val_loss: 0.0237\n",
      "Epoch 35/35\n",
      "43/43 - 0s - loss: 0.0836 - val_loss: 0.0227\n",
      "Fold score (MAE): 0.15066974638226785\n",
      "Final out of sample (oos) score (RMSE): 0.15572587599676918\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "kf = KFold(5, shuffle=True, random_state=42) # Use for KFold classification\n",
    "oos_yt= []    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "for train, test in kf.split(x):\n",
    "    fold+=1\n",
    "    print(f\"Fold #{fold}\")\n",
    "        \n",
    "    x_train = x[train]  \n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=x.shape[1], activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(35, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    \n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),verbose=2,\n",
    "              epochs=35)\n",
    "    \n",
    "    pred = model.predict(x_test)\n",
    "    oos_yt.append(y_train)\n",
    "    oos_y.append(y_test)\n",
    "    oos_pred.append(pred)    \n",
    "\n",
    "    # Measure this fold's RMSE\n",
    "    score = np.sqrt(metrics.mean_absolute_error(pred,y_test))\n",
    "    print(f\"Fold score (MAE): {score}\")\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "oos_yt=np.concatenate(oos_yt)\n",
    "\n",
    "score = np.sqrt(metrics.mean_absolute_error(oos_pred,oos_y))\n",
    "print(f\"Final out of sample (oos) score (RMSE): {score}\")    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.7/site-packages (3.1.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/anaconda3/lib/python3.7/site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (46.0.0.post20200309)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib) (1.14.0)\n",
      "[0.859467 0.121883 0.003374 0.781081 0.049411 0.018807 0.006284 0.011044\n",
      " 0.08039  0.145164 0.015475]\n",
      "[[ 0.07531247]\n",
      " [ 0.6240422 ]\n",
      " [ 0.45373878]\n",
      " [ 0.6589144 ]\n",
      " [ 0.05861497]\n",
      " [ 0.09865056]\n",
      " [ 0.2869479 ]\n",
      " [ 0.16490218]\n",
      " [ 0.06701019]\n",
      " [ 0.10189034]\n",
      " [ 0.043685  ]\n",
      " [ 0.03821816]\n",
      " [ 0.06938758]\n",
      " [ 0.13491735]\n",
      " [ 0.07145759]\n",
      " [ 0.16163549]\n",
      " [ 0.12252794]\n",
      " [ 0.23923656]\n",
      " [ 0.9135694 ]\n",
      " [ 0.8973521 ]\n",
      " [ 0.81651866]\n",
      " [-0.01333419]\n",
      " [ 0.27541718]\n",
      " [ 0.03737716]\n",
      " [ 0.7657176 ]\n",
      " [ 0.00180522]\n",
      " [ 0.90138185]\n",
      " [ 0.7507064 ]\n",
      " [ 0.04380814]\n",
      " [ 0.06479006]\n",
      " [ 0.05796118]\n",
      " [ 0.00508351]\n",
      " [ 0.09632078]\n",
      " [ 0.00423269]\n",
      " [-0.02407946]\n",
      " [-0.01845404]\n",
      " [ 0.03762025]\n",
      " [ 0.6714478 ]\n",
      " [-0.01530963]\n",
      " [ 0.4217983 ]\n",
      " [ 0.00517353]\n",
      " [-0.01638996]]\n",
      "[1.31440e-02 8.96455e-01 7.81081e-01 1.00000e+00 6.28400e-03 3.89900e-02\n",
      " 4.37870e-01 1.08834e-01 9.71000e-04 1.21883e-01 2.15010e-02 0.00000e+00\n",
      " 4.94110e-02 1.44573e-01 6.60270e-02 1.88253e-01 1.45164e-01 2.86448e-01\n",
      " 9.31283e-01 9.18234e-01 8.59467e-01 1.17950e-02 2.92268e-01 8.03900e-02\n",
      " 7.59677e-01 1.45090e-02 9.90143e-01 8.59561e-01 1.92200e-02 1.88070e-02\n",
      " 6.83740e-02 9.82300e-03 4.96920e-02 1.54750e-02 3.37400e-03 8.64000e-03\n",
      " 8.17980e-02 7.72163e-01 1.31090e-02 4.66221e-01 1.10440e-02 3.86200e-03]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "df_1=pd.read_csv(\"/Users/philipsmith/Documents/validation.csv\",sep=\",\")\n",
    "\n",
    "df_1_2=df_1['VP'].values\n",
    "\n",
    "print(df_1_2)\n",
    "print(oos_pred)\n",
    "print(oos_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (53, 1) and (212,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ab59eeb5cd30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m plt.plot(oos_pred, #training Y value\n\u001b[1;32m      2\u001b[0m  \u001b[0moos_yt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#predicted Y value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m  'ro') #red dots\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training Set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Actual'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2794\u001b[0m     return gca().plot(\n\u001b[1;32m   2795\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2796\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m         \"\"\"\n\u001b[1;32m   1664\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1665\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1666\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 270\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (53, 1) and (212,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(oos_pred, #training Y value\n",
    " oos_yt, #predicted Y value\n",
    " 'ro') #red dots\n",
    "plt.title('Training Set')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "#Compute R-Square value for training set\n",
    "testR2=r2_score(oos_pred, oos_y)\n",
    "print(\"Training Set R-Square=\", testR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.931283 0.918234 0.990143 0.859561 0.013144 0.859467 0.121883 0.003374\n",
      " 0.896455 0.021501 0.00864 ]\n",
      "[0.859467 0.121883 0.003374 0.781081 0.049411 0.018807 0.006284 0.011044\n",
      " 0.08039  0.145164 0.015475]\n"
     ]
    }
   ],
   "source": [
    "oos_pred_2=y[0:11]\n",
    "oos_pred_3=np.asarray(oos_pred_2)\n",
    "print(oos_pred_3)\n",
    "print(df_1_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set R-Square= -0.43051958335039475\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(oos_pred_2, #training Y value\n",
    " df_1_2, #predicted Y value\n",
    " 'ro') #red dots\n",
    "plt.title('Validation Set')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "#Compute R-Square value for training set\n",
    "testR2=r2_score(oos_pred_2,df_1_2)\n",
    "print(\"Validation Set R-Square=\", testR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
